    % Document Class
    \documentclass[12pt,a4paper]{article}

    % Packages essentiels
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage[french]{babel}
    \usepackage{lmodern}

    % Packages mathématiques
    \usepackage{amsmath}
    \usepackage{amssymb}
    \usepackage{amsthm}
    \usepackage{mathtools}

    % Packages pour les graphiques et figures
    \usepackage{graphicx}
    \usepackage{pgfplots}
    \pgfplotsset{compat=1.18}
    \usepackage{float}
    \usepackage{subcaption}

    % Mise en page et design
    \usepackage[hmargin=2.5cm,vmargin=2cm]{geometry}
    \usepackage{fancyhdr}
    \usepackage{enumitem}
    \usepackage{xcolor}
    \usepackage{titlesec}
    \usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
    \documentclass{article}
    \usepackage{pgfplots}
    \usepgfplotslibrary{fillbetween}
    \usepackage{caption}
    \pgfplotsset{compat=1.18}

    % Configuration des en-têtes et pieds de page
    \pagestyle{fancy}
    \fancyhf{}
    \fancyhead[L]{\slshape\nouppercase{\leftmark}}
    \fancyhead[R]{\thepage}
    \renewcommand{\headrulewidth}{0.4pt}

    % Définition des environnements mathématiques
    \newtheorem{theorem}{Théorème}[section]
    \newtheorem{proposition}[theorem]{Proposition}
    \newtheorem{lemma}[theorem]{Lemme}
    \newtheorem{corollary}[theorem]{Corollaire}
    \theoremstyle{definition}
    \newtheorem{definition}[theorem]{Définition}
    \newtheorem{example}[theorem]{Exemple}
    \theoremstyle{remark}
    \newtheorem{remark}[theorem]{Remarque}

    % Configuration des titres de sections
    \titleformat{\section}
    {\normalfont\Large\bfseries}{\thesection}{1em}{}
    \titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

    % Informations du document
    \title{\huge\textbf{Impact des news dans les Limit Order Book}}
    \author{LAFERTE Edouard \and AIAD Janis}
    \date{Juin 2024}

    \begin{document}
    \begin{titlepage}
        \begin{center}
            \vspace*{2cm}
            
            \includegraphics[width=0.4\textwidth]{École_polytechnique_signature.png}
            

            
            {\huge\bfseries Impact des news dans les\\[0.4cm] 
            Limit Order Book\par}
            
            \vspace{2cm}
            
            {\Large\textsc{Mémoire de Recherche}\par}
            \vspace{1cm}
            
            {\large
            \begin{tabular}{c}
                \textbf{LAFERTE Edouard}\\[0.2cm]
                \textbf{AIAD Janis}
            \end{tabular}\par}
            
            \vspace{1.5cm}
            
            {\large Sous la direction de\par}
            \vspace{0.4cm}
            {\large\textbf{LEHALLE Charles-Albert}\par}
            
            \vfill
            
            {\large Département de Mathématiques Appliquées\\
            École Polytechnique\\[0.4cm]
            Juin 2024\par}
        \end{center}
    \end{titlepage}

    % Page blanche après la page de titre
    \newpage
    \null
    \thispagestyle{empty}
    \newpage

    \begin{abstract}
    \thispagestyle{empty}
    \vspace*{1cm}
    \begin{center}

    \end{center}
    \vspace{1cm}

    Ce mémoire étudie l'impact des news et actualités sur la dynamique des Limit Order Books dans les marchés financiers à haute fréquence. Nous analysons comment les événements d'actualité influencent la microstructure du marché et modifient les comportements des acteurs. Notre approche combine une modélisation mathématique rigoureuse via le modèle Queue Reactive avec une analyse empirique des données de marché.

    \vspace{1cm}
    \textbf{Mots-clés :} Limit Order Book, Trading Haute Fréquence, Modèle Queue Reactive, Impact des News, Microstructure de Marché

\end{abstract}
    \newpage
    \tableofcontents
    \thispagestyle{empty}

    \newpage
    \setcounter{page}{1}
    
    \section{Limit Order Book}

    \subsection{Présentation des marchés hautes fréquences (HF)}


    L'électronification des marchés financiers à la fin du dernier siècle a été un tournant majeur dans le fonctionnement des marchés. L'adoption massive de la technologie et des systèmes électroniques s'est accompagnée d'une intensification  du nombre de trades effectués chaque jour (cf.Figure) et a eu pour conséquence de drastiquement réduire le temps d'exécution des ordres de marchés qui sont -aujourd'hui d'une fréquence de l'ordre de la dizaine de microsecondes - créant ainsi une toute nouvelle structure à l'échelle microéconomique. 
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                width=0.9\textwidth,
                height=0.4\textwidth,
                xlabel={Année},
                ylabel={Options ADV (en millions)},
                xmin=1973, xmax=2025,
                ymin=0, ymax=45,
                xtick={1975, 1980, 1990, 2000, 2010, 2020},
                ytick={0, 10, 20, 30, 40, 45},
                legend pos=south east,
                ymajorgrids=true,
                grid=both,
                grid style=dashed,
                tick label style={font=\footnotesize},
                xlabel style={yshift=-3pt},
                ylabel style={yshift=-5pt},
                xtick align=outside,
                ytick align=outside,
                tick style={major tick length=6pt, thick},
                axis line style={thick},
                enlargelimits=false,
                clip mode=individual,
                label style={font=\small},
                every tick/.style={color=black, thick},
                xticklabels={1975, 1980, 1990, 2000, 2010, 2020}, % On force l'affichage des années comme texte statique
                scaled ticks=false,
                yticklabel style={/pgf/number format/fixed}
            ]
            
            \addplot[
                color=blue,
                mark=*]
                coordinates {
                (1975, 1)
                (1980, 2)
                (1985, 3)
                (1990, 4)
                (1995, 6)
                (2000, 8)
                (2005, 10)
                (2010, 15)
                (2015, 20)
                (2020, 35)
                (2023, 45)
            };
            \addlegendentry{Volume tradé journalier}

            \end{axis}
        \end{tikzpicture}
        
        \vspace{1em}
        \textbf{\large Volume de trades d'Options ADV (expiration de 1 mois) de 1975 à 2023}
    \end{center}
    Cette redéfinition de l'échelle temporelle a amené les traders à distinguer différents niveaux dans les fréquences de trading. On peut trouver trois catégories principales de type de trading:

    \begin{itemize}
        \item \textbf{Low Frequency (LF)} : Durée de l'ordre de plusieurs mois, voire plusieurs années. Il s'agit généralement de transactions à long terme, réalisées par des investisseurs qui cherchent à maximiser leurs profits sur des échéances étendues, souvent basées sur des analyses fondamentales et la prévision de tendances économiques générales réalisé par les banques et hedge funds.

        \item \textbf{Mid Frequency (MF)} : De l'ordre de la journée, l'heure, voire la minute. Les traders à fréquence moyenne cherchent à profiter des opportunités à court terme en enlevant la non continuité temporelle des marchés visible dans les transactions hautes fréquences. 

        \item \textbf{High Frequency (HF)} : Ordre de la seconde, de la milliseconde, voire de la microseconde. Ici, les traders HF (hedge funds, EHT) cherchent des modèles pour réaliser des stratégies de trading optimal, où pour exploiter l'arbitrage des marchés.
    \end{itemize}
    \\
    Les comportements des marchés sont très différents en fonction de ces échelles de fréquence, notamment entre le HFT et le MFT/LFT. Ces différences sont principalement liées à la manière dont l'offre et la demande s'ajustent sur les échelles de temps. En particulier, en HF le temps n'est plus continu et il est nécessaire de prendre en compte cette discontinuité pour avoir une modélisation réaliste des marchés. Les modélisation sont donc fondamentalement différentes: les modèles browniens sont utilisés en grande majorité en MLF/LFT en supposant que le cours de l'actif est continu, là où des modèles de chaînes de Markov modélisant l'offre et la demande sont préférés en HFT.
    \\
    \\
    Les interactions entre les acheteurs et les vendeurs se font de manière algorithmique, selon l'ordre d'arrivée des ordres dans les files d'attente de l'offre et de la demande. Ce processus est géré par le \textbf{Limit Order Book}, le carnet d'ordres, qui liste et exécute tous les ordres envoyés par les différents acteurs du marché. Ces modifications sont de trois natures principales :

    \begin{itemize}
        \item \textbf{Limit Order} : ajout d'une proposition d'achat ou de vente à un prix déterminé. 

        \item \textbf{Market Order} : Achat ou vente immédiat au meilleur prix disponible sur le marché. 

        \item \textbf{Cancellation} : Retrait d'une proposition d'achat ou de vente précédemment inscrite dans le carnet d'ordres.
    \end{itemize}

    La microstructure de marché créée via ces interactions entre acheteurs et vendeurs est à la base du processus de formation des prix et entraîne leurs variations au cours du temps.

    \subsection{Processus de formation des prix}
    \subsubsection{Le Limit Order Book}
    À l'échelle haute fréquence, les mouvements des prix découlent en grande partie des interactions entre l'offre et la demande. De chaque côté (bid et ask), les acteurs font des offres plus ou moins proches du prix de référence $p_{ref}$. L'une des caractéristiques principales de la HF est que les prix ne peuvent prendre que des valeurs multiples d'une grandeur appelée \textbf{tick}. 
    
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            % Axis
            \begin{axis}[
                width=12cm,
                height=4.5cm,
                axis x line=middle,
                axis y line=left,
                ymin=0,
                ymax=300,
                xmin=-5,
                xmax=5,
                xtick={-4,-3,-2,-1,0,1,2,3},
                xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
                xlabel={Price ticks},
                ylabel={Taille de la file},
                ymajorgrids,
                tick label style={font=\footnotesize},
                xlabel style={below},
                ylabel style={above}
            ]
    
            % Bid side
            \addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
            {(-4,260) (-3,250) (-2,200) (-1,100)};
    
            % Ask side
            \addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
            {(0,89) (1,175) (2,198) (3,220)};
    
            % Reference price line
            \draw[thick, dashed] (axis cs:-0.5,-0.5) -- (axis cs:-0.5,220);
            \node[anchor=south] at (axis cs:-0.5,230) {$p_{ref}$};
    
            % Annotations for sides
            \node[above] at (axis cs:-2.5,250) {\textbf{Bid Side}};
            \node[above] at (axis cs:1.5,250) {\textbf{Ask Side}};
    
            % One tick annotation
            \draw[<->] (axis cs:-0.2,-10) -- (axis cs:0.2,-10);
            \node[below] at (axis cs:0,-10) {One Tick};
    
            \end{axis}
        \end{tikzpicture}
        \caption{Limit order book théorique}
    \end{figure}
Ainsi, la différence bid-ask ne peut prendre que des valeurs entières de ticks et sont situés à une distance plus ou moins proche de $p_{ref} = \frac{p_{bid}+p_{ask}}{2}$. À chaque instant, de nouveaux acteurs peuvent s'ajouter sur un prix, qui sera alors modélisée comme une file d'attente, dont l'ordre est défini par l'ordre d'arrivée. Le graphique ci-dessus est une représentation  théorique des tailles de file d'attente d'un actif. On voit que les tailles sont de plus en plus grande, et théoriquement devraient augmenter de 0 jusqu'à $+\infty$, un acheteur ou un vendeur ayant plus intérêt à se mettre le plus loin de $p_{ref}$, pour augmenter ses bénéfices. Cela n'est pas observé en pratique, vu que se placer dans une queue est payant et l'on a donc une limite à partir de laquelle l’espérance des gains devient plus faible que le coût d'entrée.


\hspace*{-2.5cm}
\begin{tikzpicture}[
    every node/.style={align=center},
    node distance=2cm and 4cm,
    >={Stealth}
]

\node (orderbook) {};
\begin{axis}[
    at={(0,0)},
    anchor=west,
    width=10cm,
    height=7cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ylabel={Taille de la file d'attente},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title={État à $t_k$},
    at={(0,0)}
]
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,130) (-3,100) (-2,50) (-1,10)};
\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,175) (1,198) (2,220) (3,230)};
\draw[thick, dashed] (axis cs:-0.5,-10) -- (axis cs:-0.5,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}

\node[below right=of orderbook, yshift=6.5cm, xshift=10cm] (cancel) {\textbf{}};
\begin{axis}[
    at={(cancel.east)},
    anchor=west,
    width=9cm,
    height=6.5cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title = {Add de 20 à l'Ask}
]
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,130) (-3,100) (-2,50) (-1,30)};
\addplot[ybar, fill=blue!10, draw=none, bar width=0.7] coordinates
{(-1,10)};
\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,175) (1,198) (2,220) (3,230)};
\draw[thick, dashed] (axis cs:-0.5,-10) -- (axis cs:-0.5,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}

\node[below right=of orderbook, yshift=0cm, xshift=10cm] (add) {\textbf{}};
\begin{axis}[
    at={(add.east)},
    anchor=west,
    width=9cm,
    height=6.5cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title={Cancel de 10 au Bid}
]
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,130) (-3,100) (-2,50) (-1,10)};
\addplot[ybar, fill=blue!10, draw=none, bar width=0.7] coordinates
{(-1,10)};
\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,175) (1,198) (2,220) (3,230)};
\draw[thick, dashed] (axis cs:-1,-10) -- (axis cs:-1,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}

\node[below right=of orderbook, yshift=-6.5cm, xshift=10cm] (order) {\textbf{}};
\begin{axis}[
    at={(order.east)},
    anchor=west,
    width=9cm,
    height=6.5cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title={Order de 60 au Bid}
]
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,130) (-3,100) };
\addplot[ybar, fill=blue!10, draw=none, bar width=0.7] coordinates
{(-2,50) (-1,10)};
\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,175) (1,198) (2,220) (3,230)};
\draw[thick, dashed] (axis cs:-1.5,-10) -- (axis cs:-1.5,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}
\draw[dashed, thick] (9,-9) -- (9,9);
\node at (4,10) {\textbf{Limit Order Book à $t_k$}};
\node at (13,10) {\textbf{Limit Order Book à $t_{k+1}$}};
\node at (8,-10) {\textbf{Évolutions possibles du Limit Order Book}};
\end{tikzpicture}
\\
\\
On a représenté ci-dessus l'évolution du MBO suite à un Add, Cancel ou un Order. On voit bien que l'on peut avoir une évolution du prix de référence dans le cas où l'on vient à épuiser une limite où lorsqu'une nouvelle est créée. C'est cette dynamique du prix de référence qu'il faut capter afin de bien modéliser l'évolution des prix en haute fréquence pour l'entraînement de stratégies de trading. Pour bien comprendre l'évolution et les interactions entre l'offre et la demande, nous utiliserons des données brutes de carnets d'ordre dans toute notre étude. Nous commencerons par exploiter simplement les données avant d'étudier les modélisation des carnets d'ordres en HF.


\subsubsection{Données utilisées}
Dans la suite de notre étude, nous étudierons tout particulièrement les actifs GOOGL, LCID, et KHC. Ces actifs correspondent respectivement à Google, Lucid Group Inc (constructeur américain de véhicules électriques de luxe) et The Kraft Heinz Company (multinationale américaine spécialisée dans l'agroalimentaire).

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Actif}& \textbf{Nb Timestamps} & \textbf{Nb Add} & \textbf{Nb Cancel} & \textbf{Nb Order}\\ \hline
GOOGL  & 86386103         & 81146187        & 6448095    & 173980437  \\ 
\hline
LCID       & 2101840         & 1975378         & 522436  & 4599687  \\
\hline
KHC   & 6300128   & 5982598 & 909555 & 13192289                   
\\ \hline
\end{tabular}
\caption{Tableau des données du Nasdaq sur 65 jours du 28 juillet au 4 novembre}
\label{tab:exemple}
\end{table}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Actif}& \textbf{Nb/Jour Actions}& \textbf{Pct Add} & \textbf{Pct Cancel} & \textbf{Pct Order}\\ \hline
GOOGL & 2676622 & 49,65\%  & 46,64\%     & 3,70\%    \\ 
\hline
LCID  & 70764   & 45,69\%  & 42,94\%    &11,35\%    \\
\hline
KHC  & 202958   & 47,75\%  & 45,34\%  & 6,89\%                     
\\ \hline
\end{tabular}
\caption{Tableau des données du Nasdaq moyennées sur 65 jours}
\label{tab:exemple}
\end{table}
Les données sont issus de la base de données Databento et ne concernent que la bourse du Nasdaq. On voit tout d'abord que certains actifs sont plus tradés que d'autres notamment GOOGL par rapport au deux autres. Les estimations seront donc naturellement plus précise pour cet actif.
\\
\\
Les données sont sous format de dataframe par journée comprenant toutes les modifications apportées au MBO des trois actifs étudiés. Ce MBO comprend alors:
\begin{itemize}
    \item ts\_recv: timestamp du serveur 
    \item ts\_event: timestamp de l’événement     
    \item rtype: type d’événement     
    \item publisher\_id: id du publisher du MBO (2 pour Nasdaq,...)
    \item instrument\_id: id de l'actif sur les marchés
    \item action: Add(A), Cancel(C), Trade(T)
    \item side: Ask(A), Bid(B)
    \item depth: limite considéree
    \item price: prix
    \item size: taille de l'action réalisée
    \item ts\_in\_delta: temps entre le serveur et l'action
    \item sequence: id de l'action propre à l'actif sur les marchés
    \item bid\_px\_0x: prix à la limite x côté bid
    \item ask\_px\_0x: prix à la limite x côté ask
    \item bid\_sz\_0x: taille de la file d'attente à la limite x côté bid
    \item ask\_sz\_0x: taille de la file d'attente à la limite x côté ask
    \item symbol: symbole de l'actif
\end{itemize}


Nous avons dans un premier temps trier ces données, qui étaient très dégradées. Pour commencer, afin de s'enlever les comportements particuliers de l'ouverture et de la fermeture des marchés, nous avons choisi de ne sélectionner que les données à $\pm1.5$h de la fermeture/ouverture de la bourse du Nasdaq. Par la suite nous avons réalisé un traitement systématique des données, afin d'enlever les occurrences inutiles.
\\
\\
Dès le début de notre étude, nous avons observé que plusieurs actions avaient lieu au même timestamp, ce qui n'est bien sûr pas réaliste. Afin de régler ce problème, nous avons observé les suites d'actions que cela concernait:
\begin{itemize}
\item Séquences de Trades terminées par un Cancel sans épuisement de limite: Ces actions correspondent probablement à un Trade agressif de type iceberg, qui est découpé en plus petit trades. Dans ce cas, nous avons concaténé les trades pour n'en faire qu'un seul dont la taille est la somme de tous les trades réalisés. On observe que cette suite d'actions finie souvent par un Cancel (mais pas tout le temps);
\item Séquences de Trades terminées par un Cancel avec épuisement de limite: Dans ce cas, l'ordre de trade agressif épuise la limite. Nous avons donc comme précédément concaténé les trades avant et après épuisement de la limite tout en marquant le passage d'une nouvelle limite. Le Cancel correspond ici probablement à l'annulation des trades qui devaient avoir lieu sur la limite épuisée;
\item Séquences de Trades et de Cancel terminées par un Cancel sans épuisement de limite: On observe également des séquences de trades agressifs qui ne se terminent pas par un Cancel. Nous l'avons traité comme précédemment en regroupant les trades en un seul;
\item Séquences de Trades et de Cancel terminées par un Cancel avec épuisement de limite: Même traitement que précédemment;
\item Séquences de Trades et de Cancel non terminées par un Cancel sans épuisement de limite: Même traitement que précédement.
\end{itemize}
Ces observations pourraient être expliquées par le fait qu'un trade agressif de taille $N$ décomposé en $m$ petits trades est process par paquets d'une dizaine de trades suivi d'un cancel par exemple.
\\
\\
Afin de vérifier que notre dataframe était bien nettoyé, nous regardons si la size de l'action à l'instant $t_k$ est en accord avec la tailles des queues aux instants $t_k$ et $t_{k+1}$. Nous devrions alors avoir un marqueur positif à l’exception des timestamps correspondant à l'apparition ou la disparition d'une limite. Pour cela, l'on sélectionne la limite $N$ que l'on veut étudier tout en prenant soin de prendre en compte les tailles des queues $N-1$ et $N+1$ en compte pour l'apparition des nouvelles limites. Nous avons alors nos données totalement nettoyées par limite. 
\\
\\
Avec les données obtenues, on peut dans un premier temps tracer les limites et l'évolution du prix en fonction du temps. Le graphique obtenu nous montre bien que les trades n'ont pas lieu à intervalle de temps régulier, et semblent plus fréquent dans les zones où les prix varient beaucoup.
On peut également s'intéresser à la taille moyenne des queues sizes, afin d'observer la décroissance de la taille des queues annoncée précédemment. On observe bien que les tailles moyenne de queues commencent par augmenter puis diminuent de part et d'autre du bid-ask. On a donc bien le comportement attendu.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Prix&limit.png}
    \caption{Évolution du prix de GOOGL avec le bid-ask le 2024-07-25 entre $16:02:00$ et $16:03:00$. Les points noirs correspondent aux trades, les tracés correspondent aux limites de -9 à +9}
    \label{fig:graph}
\end{figure}

    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            % Axis
            \begin{axis}[
                width=16cm,
                height=8cm,
                axis x line=middle,
                axis y line=left,
                ymin=0,
                ymax=400,
                xmin=-11,
                xmax=11,
                xtick={-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9},
                xticklabels={$P_{-10}$, $P_{-9}$, $P_{-8}$, $P_{-7}$, $P_{-6}$, $P_{-5}$,$P_{-4}$,$P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$,$P_5$,$P_6$,$P_7$,$P_8$, $P_9$, $P_{10}$},
                xlabel={Price ticks},
                ylabel={Taille de la file d'attente},
                ymajorgrids,
                tick label style={font=\footnotesize},
                xlabel style={below},
                ylabel style={above}
            ]
    
            % Bid side
            \addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
            {(-1,151.1224671373919 ) (-2,219.53969758685668) (-3,295.5069691067028) (-4,335.0593228757234) (-5,338.5736082586511)  (-6,331.6095311018006) (-7,322.0771596238568) (-8,312.9499932179665) (-9,307.58115194730084) (-10,303.5522045138593)};
    
            % Ask side
            \addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
            {(0,138.55837238255106) (1,192.0643778858309) (2,243.8878344941788) (3,283.3784230990515) (4,303.12025200927195)  (5,296.46381873364055) (6,282.8737240826732) (7,280.2261308977849) (8,274.74242139270194) (9,271.95192471085596)};
    
            % Reference price line
            \draw[thick, dashed] (axis cs:-0.5,-0.5) -- (axis cs:-0.5,220);
            \node[anchor=south] at (axis cs:-0.5,230) {$p_{ref}$};
    
            % Annotations for sides
            \node[above] at (axis cs:-6,350) {\textbf{Bid Side}};
            \node[above] at (axis cs:5,350) {\textbf{Ask Side}};
    
            % One tick annotation
            \draw[<->] (axis cs:-0.2,-10) -- (axis cs:0.2,-10);
            \node[below] at (axis cs:0,-10) {One Tick};
    
            \end{axis}
        \end{tikzpicture}
        \caption{Taille moyenne des files d'attente du Limit order book de GOOGL le 2024-07-25}
    \end{figure}
\\
\\
Nous allons maintenant étudier une des grandeurs caractéristique de la haute fréquence, qui permet de capter la majeure partie des mouvements de prix en suivant le MBO: l'imbalance.

\subsection{L'imbalance, la clé des modèles hautes fréquences}

L'étude de la première limite du MBO (Bouchaud et al., 2004 et Besson et al., 2016) permet de comprendre comment la taille de la première limite du bid et de l'ask influent sur le prochain mouvement de prix. Malheureusement, cette prédiction n'est pas assez précise pour être rentable sur le long terme, et il est donc nécessaire d'avoir une étude plus fine du processus de formation des prix. Cependant, l'étude de la première limite nous permet de sortir une première information capitale de l'order book, l'imbalance que l'on défini par:
$$\text{Imb}_t = \frac{Q^{best\ bid}_t-Q^{best\ ask}_t}{Q^{best\ bid}_t+Q^{best\ ask}_t}$$
Cette première donnée permet de capter les mouvements de prix moyen. En effet, lorsque l'imbalance est négatif, le déséquilibre entre l'offre et la demande aura pour conséquence de faire évoluer les prix vers le bas, et inversement. 
\begin{figure}[h!]
    \centering
    % Première figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Imbalance.png}
        \caption{Imbalance en fonction du Delta tick}
        \label{fig:imbalance}
    \end{subfigure}
    \hfill
    % Deuxième figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{densité_Imbalance.png}
        \caption{Courbe de distribution de l'imbalance}
        \label{fig:densite_imbalance}
    \end{subfigure}
    \caption{Imbalance des trades de KHC avec avec intervalle de confiance à 95\%}
    \label{fig:comparison}
\end{figure}
\\
En traçant l'imbalance en fonction du $\Delta_p$, on observe une relation linéaire entre ces deux valeurs, qui nous conforte donc sur nos observations précédentes. On remarque aussi que l'imbalance n'a pas une densité uniforme, ce qui revient à dire que le Limit Order Book est plus souvent déséquilibré qu'équilibré, ce qui permet d'avoir des mouvements de prix. Nous verrons par la suite d'autres observations sur l'imbalance notamment en fonction de l'intensité des trades.
\\
\\
L'étude de l'imbalance peut aussi se porter sur les add et les cancels, qui ont des comportements similaires, n'étant pas des orders.
\begin{figure}[h!]
    \centering
    % Première figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{densité_add.png}
        \caption{Add}
        \label{fig:imbalance}
    \end{subfigure}
    \hfill
    % Deuxième figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{densité_cancel.png}
        \caption{Cancel}
        \label{fig:densite_imbalance}
    \end{subfigure}
    \caption{Courbes de distribution de l'imbalance des add et cancel de KHC avec intervalle de confiance à 95\%}
    \label{fig:comparison}
\end{figure}
\\
On observe ici un comportement inversé aux trades: un acteur aura tendance à s'ajouter ou à se retirer lorsqu'il ne se passe rien en prévision d'un trade aux imbalances extrêmes. Une étude aux limites supérieures nous montre que le comportement de l'imbalance est relativement similaire, avec une distribution qui devient de plus en plus uniforme au fur et mesure que l'on s'enfonce dans les limites.
\\




\clearpage
\section{Modèle Queue Reactive}

Nous commencerons par aborder la modélisation Queue Reactive dans un monde où il n'existe qu'un seul prix. Nous complexifierons le modèle au fur et à mesure.
\\
\\
L'étude des Order Book des marchés HFT a amené à une modélisation par file permettant une explication fine du processus de création des prix. Prenons l'exemple d'un prix fixe appelé middle price $p_{\text{mid}}$. L'offre et la demande orbitent autour de ce prix avec une offre qui se trouve toujours au dessus du prix et une demande toujours en dessous dans un marché sans arbitrage. La loi international des marchés financier n'autorise à l'acheteur et au vendeur que certains prix séparés par une distance appelée $\textbf{un tick}$. Dans le modèle QR, les acheteurs et les vendeurs sont ainsi modélisés comme des queues à une distance plus ou moins proche du prix $p_{\text{mid}}$, qui est en pratique le prix du marché. Ces queues permettent au vendeurs de choisir le meilleur prix et à l'acheteur d'acheter au prix le plus bas. Prenons l'exemple d'un actif fictif à 100\$ avec un tick de 0.1\$. Les acheteurs seront répartis dans les queues $Q_{-1},Q_{-2},...$. Il y aura ainsi 90 acheteurs à $Q_{-1}$ avec un prix de 99.9\$, 100 à $Q_{-2}$ avec un prix de 99.8\$, ect... Le même modèle sera utilisé pour les vendeurs eux répartis dans les $Q_{1},Q_2,...$. Les queues sont réactualisées à chaque instant en consommant un AES(average event size) ou en ajoutant un AES avec une fréquence suivant une loi de poisson de paramètre $\lambda^{Q_\pm i}_{\text{Action}}(\text{taille de la queue } Q_\pm i)$. Ainsi le carnet d'offre et de demande est une chaîne de markov, dont l'état à l'instant $t_{i+1}$ ne dépend que de celui à l'instant $t_i$.

\subsection{Modèle QR à prix fixe}
On note $p_{ref}$ le prix fixe de référence. On va modéliser l'évolution des queues autour de ce prix. On modélise le carnet d'ordre par un vecteur: $$Q(t) = (Q_{-K}(t),...Q_{-1}(t),Q_{1}(t),Q_K(t))$$ qui évolue dans le temps selon un processus de Markov. L'élément $Q_{\pm i}(t)$ correspond à la disponibilité de l'offre ou de la demande à $p_{ref}\pm i \ \text{tick}$ au temps $t$.
\\
\\Trois modifications aboutissent à un changement de $Q$:
\begin{itemize}
    \item \textbf{Limit order}: insertion d'une demande d'achat "bid" $(i<0)$ où d'une demande de vente "ask" $(i>0)$.
    \item \textbf{Cancellation order}: suppression d'une demande ou d'un ask sans trade.
    \item \textbf{Market Order}: suppression d'une demande ou d'un ask avec trade.
    \end{itemize}
À la date $t$, la queue $Q_{\pm}$ peut donc soit augmenter (Limit order) soit diminuer (Cancellation, Market order).
\\
\\ 
Supposons que nous sommes à la date $t$, avec le vecteur: $$Q(t) = (Q_{-K}(t),...Q_{-1}(t),Q_{1}(t),Q_K(t))$$ Pour modéliser le prochain mouvement du MB, on tire $6K$ variables aléatoires de Poisson indépendantede paramètres respectifs $\lambda_i^{U}(Q_i(t))$ avec $U\in \{Add,Cancel,Trade\}$. La prochaine action sera alors choisie comme étant un AES (average event size) d'action $U_{min}$ à la queue $Q_{i_{min}}$ achevant le minimum sur ces variables de Poisson:$$(i_{min},U_{min}) = \underset{(i,U) \in \{-K,K\}\times\{A,C,T\}}{\text{argmin}}\ \mathcal{P}(\lambda_i^{U}(Q_i(t)))$$
L'action sera réalisé à la date $t+\mathcal{P}(\lambda_{i_{min}}^{U_{min}}(Q_{i_{min}}(t)))$.
Afin de garantir d'avoir plus d'ajout que de consommation, on oblige les intensités des add à être plus faibles que celles des cancels et des orders sommées.
\\
\\
\hspace*{-1.5cm}
\begin{tikzpicture}[
    every node/.style={align=center},
    node distance=2cm and 4cm,
    >={Stealth}
]

\node (orderbook) {};
\begin{axis}[
    at={(100,0)},
    anchor=west,
    width=9cm,
    height=6.5cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ylabel={Taille de la file d'attente},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title={Tirage des variables aléatoires de Poisson},
    at={(0,0)}
]
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,220) (-3,200) (-2,160) (-1,100)};
\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,160) (1,180) (2,200) (3,220)};
\draw[thick, dashed] (axis cs:-0.5,-10) -- (axis cs:-0.5,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}


\node[below right=of orderbook, yshift=0cm, xshift=10cm] (add) {\textbf{}};
\begin{axis}[
    at={(1100,0)},
    anchor=west,
    width=9cm,
    height=6.5cm,
    axis x line=middle,
    axis y line=left,
    ymin=0, ymax=300,
    xmin=-5, xmax=4,
    xtick={-4,-3,-2,-1,0,1,2,3},
    xticklabels={$P_{-4}$, $P_{-3}$, $P_{-2}$, $P_{-1}$, $P_1$, $P_2$, $P_3$, $P_4$},
    xlabel={},
    ymajorgrids,
    tick label style={font=\footnotesize},
    xlabel style={below},
    ylabel style={above},
    title={$(U_{min},i_{min}) =( A, -2)$ et $\text{AES} = 20$ }
]
\addplot[ybar, fill=blue!10, draw=none, bar width=0.7] coordinates
{(-2,180)};
\addplot[ybar, fill=blue!60, draw=none, bar width=0.7] coordinates
{(-4,220) (-3,200) (-2,160) (-1,100)};

\addplot[ybar, fill=blue!40, draw=none, bar width=0.7] coordinates
{(0,160) (1,180) (2,200) (3,220)};
\draw[thick, dashed] (axis cs:-0.5,-10) -- (axis cs:-0.5,250)
    node[pos=0.95, above] {$p_{ref}$};
\end{axis}
\node at (4,4) {\textbf{Limit Order Book à $t_k$}};
\node at (13,4) {\textbf{Limit Order Book à $t_k+\mathcal{P}(\lambda_{i_{min}}^{U_{min}}(Q_{i_{min}}(t)))$}};
\node at (8,-4) {\textbf{Exemple d'évolution possible du Limit Order Book entre $t_k$ et $t_{k+1}=t_k+\mathcal{P}(\lambda_{i_{min}}^{U_{min}}(Q_{i_{min}}(t)))$}};
\end{tikzpicture}
\\
\\
Cette modélisation peut être améliorée en augmentant les conditionnements sur les limites, par exemple en considérant la taille de la queue opposée ou encore les tailles de queues les plus proche du $p_{ref}$.
\\
\\
Cette modélisation à l'avantage d'être facilement calibrable sur les marchés. En effet, l'indépendance des variables de Poisson permet de calculer explicitement chaque intensité pour chaque limite, qui dépendra alors de la taille cette limite et de l'action.
\\
\\
Dans notre étude nous considérons que l'ask et le bid sont symétriques et donc qu'il n'y a donc pas de drift. Pour estimer notre intensité on rappelle dans un premier temps que l'espérance d'une variable aléatoire de Poisson est égale à $\frac{1}{\lambda}$. Les variables aléatoires de Poisson étant indépendantes par construction, on peut donc les utiliser en utilisant la loi forte des grands nombres. Pour cela, on regarde à chaque instant $t$ où l'on a modifié la limite $\pm i$ avec l'action $U$ et où la taille à la date $t$ était de $T_t^{i}$ et le delta de temps entre l'événement $t$ et le suivant est noté $\Delta_t$. On a alors en utilisant le maximum de vraisemblance en notant $e$ un événement du LOB: 
$$\hat\lambda_i^U(T^{i})= \frac{\#\{e\in Q_i\cap U\}}{\#\{e\in Q_i\}}\left(\frac{1}{\#\{e\in Q_i\}}\sum_{\{e\in Q_i\}}\Delta_e\right)^{-1}$$ce qui correspond à la probabilité d'avoir l'action $U$ multipliée par la fréquence moyenne des événements à la limite $i$ lorsque qu'elle est de taille $T^{i}$. L'intervalle de confiance est l'intervalle classique obtenu avec le théorème central limite.
\\
\\ 
En pratique les prix ne sont pas fixes, et on choisit donc de considérer que ces limites ne dépendent pas du prix. On peut donc recommencer le processus en recomptant après chaque changement de $p_{ref}$.

\subsection{Modèle QR à prix variable}
Il reste encore à modéliser les mouvements du prix $p_{ref}$. Pour cela, on considère que le prix $p_{ref}$ ne peut bouger que sous deux conditions:
\begin{itemize}
    \item épuisement d'un limite
    \item ajout d'une nouvelle limite
\end{itemize}
En toute vraisemblance, le prix $p_{ref}$ devrait évoluer dans le sens du marché, c'est à dire monter quand une limite s'épuise au niveau de l'ask par exemple. Cependant, ce comportement n'est pas observé sur les marché et c'est un phénomène de mean-reversion qui est prédominant. Afin de capter cette évolution, à chaque changement du prix $p_{ref}$, on tire une loi binomiale de paramètre $\theta$ qui vaudra 1 si le prix suit le mouvement de marché et 0 si sa direction est opposée.
\hspace*{-7.5cm}

\begin{tikzpicture}[
    every node/.style={align=center},
    node distance=2cm and 4cm,
    >={Stealth}
]

% Style des barres Bid / Ask
\tikzset{
    bidbar/.style={ybar, fill=blue!60, draw=none, bar width=0.07},
    askbar/.style={ybar, fill=blue!40, draw=none, bar width=0.07},
}

% Première étape (état initial)
\node (start){
\begin{tikzpicture}
\begin{axis}[
    width=5cm, height=4cm,
    ymin=0, ymax=40,
    xmin=99.7, xmax=100.2,
    xtick={99.8,99.9,100,100.1},
    xticklabels={$Q_{-2}$, $Q_{-1}$, $Q_1$, $Q_2$},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    axis x line=bottom, axis y line=left,
    ylabel={Quantité}, ylabel style={ xshift=-5pt},
    xlabel={}, xlabel style={ yshift=5pt},
    title={État initial}, title style={}
]
% Bid bars (vert) à gauche
\addplot[bidbar] coordinates {(99.8,30) (99.9,10) };
% Ask bars (rouge) à droite
\addplot[askbar] coordinates {(100,25) (100.1,22)};

% Ligne pmid (ici on suppose pmid=99)
\draw[dashed, thick] (axis cs:99.95,0) -- (axis cs:99.95,30) node[pos=1, above] {$p_{ref}$};

\end{axis}
\end{tikzpicture}
};

% Deuxième étape (après une opération, par exemple on ajoute au bid)
\node (mid) [right=of start, yshift=0cm, xshift=3cm] {
\begin{tikzpicture}
\begin{axis}[
    width=5cm, height=4cm,
    ymin=0, ymax=40,
    xmin=99.7, xmax=100.2,
    xtick={99.8,99.9,100,100.1},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    xticklabels={$Q_{-2}$, $Q_{-1}$, $Q_1$, $Q_2$},
    axis x line=bottom, axis y line=left,
    ylabel={Quantité}, ylabel style={ xshift=-5pt},
    xlabel={}, xlabel style={yshift=5pt},
    title={État modifié}, title style={}
]

% On imagine qu'on a ajouté une certaine quantité au niveau de 99 côté bid
\addplot[bidbar] coordinates {(99.8,30) (99.9,0)};
\addplot[askbar] coordinates {(99.9,0) (100,25) (100.1,22)};

\draw[dashed, thick] (axis cs:99.95,0) -- (axis cs:99.95,30) node[pos=1, above] {$p_{ref}$};

\end{axis}
\end{tikzpicture}
};

% Flèche entre les deux
\draw[->, thick] (start) -- (mid);

% Branching en fonction de B(\theta)
% On crée deux noeuds finaux : top pour B(θ)=0, bottom pour B(θ)=1
\node (topfinal) [right=of start, yshift=3cm, xshift=9cm]  {
\begin{tikzpicture}
\begin{axis}[
    width=5cm, height=4cm,
    ymin=0, ymax=40,
    xmin=99.7, xmax=100.2,
    xtick={99.8,99.9,100,100.1},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    xticklabels={$Q_{-1}$, $Q_{1}$, $Q_2$, $Q_3$},
    axis x line=bottom, axis y line=left,
    ylabel={Quantité}, ylabel style={xshift=-5pt},
    xlabel={}, xlabel style={yshift=5pt},
    title={$B(\theta)=1$}, title style={}
]

% Hypothèse : B(θ)=0 déplace p_ref ou modifie les quantités différemment
\addplot[bidbar] coordinates {(99.8,30)};
\addplot[askbar] coordinates {(99.9,0) (100,20) (100.1,22)};

\draw[dashed, thick] (axis cs:99.85,0) -- (axis cs:99.85,30) node[pos=1, above] {$p_{ref}$};

\end{axis}
\end{tikzpicture}
};

\node (bottomfinal) [right=of start, yshift=-3cm, xshift=9cm] {
\begin{tikzpicture}
\begin{axis}[
    width=5cm, height=4cm,
    ymin=0, ymax=40,
    xmin=99.7, xmax=100.2,
    xtick={99.8,99.9,100,100.1},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    xticklabels={$Q_{-2}$, $Q_{-1}$, $Q_1$, $Q_2$},
    axis x line=bottom, axis y line=left,
    ylabel={Quantité}, ylabel style={ xshift=-5pt},
    xlabel={}, xlabel style={yshift=5pt},
    title={$B(\theta)=0$}, title style={}
]

\addplot[bidbar] coordinates {(99.8,30) (99.9,0)};
\addplot[askbar] coordinates {(100,25) (100.1,22)};

\draw[dashed, thick] (axis cs:99.95,0) -- (axis cs:99.95,30) node[pos=1, above] {$p_{ref}$};

\end{axis}
\end{tikzpicture}
};

% Flèches de branching
\draw[->, thick] (mid.east) -- (topfinal.west);
\draw[->, thick] (mid.east) -- (bottomfinal.west);

% Labels B(theta)
\end{tikzpicture}
\\
\\
Les premières observations réalisées par C-A. Lehalle et M. Rosembaum, ont montré que ce modèle ne permettait pas de retrouver la volatilité effective des marchés. En effet, même en supposant un QR totalement drivé par le LOB, c'est à dire $\theta=0$, la volatilité n'atteignait pas la valeur observée en pratique. Pour palier à ce problème, on rajoute un autre paramètre $\theta_{réinit}\approx0.13$. Désormais, à chaque fois que le prix $p_{ref}$ bouge, on a avec probabilité $\theta_{réinit}$, une annulation de la totalité du LOB et la création d'un nouveau à partir de la loi invariante autour d'un nouveau prix $p_{ref}$. Cette nouvelle dynamique permet de modéliser les informations exogènes à notre système qui influent sur son évolution.




\subsection{Résultats théoriques sur la probabilité d'un événement rare}

On commencera par caractériser mathématiquement un évènement rare. Nous disposons d'un carnet d'ordre caractérisé par la taille des queues à chaque évent. On peut ainsi tirer de ces données trois caractéristiques principales, qui nous permettent de construire le vecteur d'état $\eta_{t\in \{t_1,t_2,\dots,t_n\}}$ avec $\{t_1,t_2,\dots,t_n\}\in [0,T_f]^n$ les temps des évents:
$$\eta_t = (\text{Imb}_t, \Delta_t, {A_t})$$
avec $\text{Imb}_t$, l'imbalance à la date $t$, $\Delta_t$ le temps entre l'évent $t$ et le suivant et $A_t\in \{A,C,T\}$ l'action réalisée à la date $t$. Nous séléctionons dans ces vecteurs ceux qui sont des trades. Nous avons donc la famille de taille $n_T$ de vecteurs de deux dimensions $\eta_t^T = (\text{Imb}_t, \Delta_t)$.
Ce vecteur nous permet de construire la statistique de test et de caractériser complètement les évents rares.
\\
\\
Pour cela, on note $\mathcal I= \{\mathcal I^1,\dots,\mathcal I^m\}$, des buckets de taille $\frac{2}{m}$ de l'intervalle $[-1,1]$. Sachant que la distribution des intensités n'est pas la même en fonction de l'imbalance (cf. Partie III), nous normalisons les $\Delta_t$ par les $\mu_i^n = \frac{1}{\#\{j\in \mathcal I^i\}}\sum_{j\in \mathcal I^i}^n\Delta_{t_i}$, qui correspondent à la moyenne de temps par bucket d'imbalance.
On note la famille $\tilde \eta_t^T = \left(\text{Imb}_t, \frac{\Delta_t}{\sum_{i=1}^m\mu_i^n\mathbb{I}_{\text{Imb}_t\in\mathcal{I}^{i}}}\right)$, qui correspond aux vecteurs moyennés par bucket d'imbalance.
Nous obtenons alors la valeur $\Delta_T^{n_T}(\alpha)$ qui correspond au quantile d'ordre $1-\alpha$ de la famille: les $\alpha\% \ \Delta_t$ les plus petits de notre série.
\\
\begin{proposition}[Convergence du quantile]
\\
La valeur $\Delta_T^{n_T}(\alpha)$ tend vers une constante $\Delta_T(\alpha)$ qui ne dépend plus de $T_f$:
$$T_f\underset{n\to +\infty}{\to}+\infty \ \text{p.s}$$
$$\frac{n_T}{n}\underset{n\to +\infty}{\to}C^{ste} \ \text{p.s}$$

$$\Delta_T^{n_T}(\alpha)\underset{n\to +\infty}{\to}\Delta_T(\alpha) \ \text{p.s}$$
\end{proposition}
\\
\textit{Démonstration}
\\
On commence par remarquer que $\eta_t^T$ est presque une chaîne de Markov, au détail près que les $\mu_i^n$, ne dépendent de 
$n$.  Pour transformer cette chaîne en chaîne de Markov, on considère le modèle QR défini précédement que l'on prolonge sur $(t_i)_{i\in\mathbb{N}}$, en conservant nos événements sur les $n$ premiers instants. On à donc ici défini une chaîne irréductible positive dont la restriction aux $n$ premières valeurs sont les mêmes que notre famille de départ. Ce n'est toujours pas un chaîne de Markov, vu que les $\mu_i^n$, dépendre de $T_f$ (et donc de $n$). Commençons donc par montrer que l'on peut remplacer les $\mu_i^n$ par $\mu_i^{\infty}$ quand $n$ tend vers $+\infty$.
\\
\\
Pour cela il faut montrer que:
$$ \ \forall i,  \mathbb{P}\left(\forall M \in \mathbb{N}, \text{Imb}_{t_i\in\{t_1,\dots,t_M\}}\notin \mathcal{I}^{i}\right) = 0$$
\\
Ce résultat est direct par construction de notre chaîne qui est désormais irréductible positive. On a donc une probabilité non nulle d'appartenir à un bucket d'imbalance, c'est-à-dire que:

$$\mu_i^n = \frac{n}{\#\{j\in \mathcal I^i\}}\frac{1}{n}\sum_{j}^n\mathbb{I}_{j\in \mathcal I^i}\Delta_{t_i}\underset{n\to+\infty}{\to}\mu_i^{\infty}$$
via la loi forte des grand nombre. Ainsi, on peut montrer notre résultat sur la famille:
$$\eta_t^{\infty} = \left(\text{Imb}_t, \frac{\Delta_t}{\sum_{i=1}^n\mu_i^{\infty}\mathbb{I}_{\text{Imb}_t}}, A_t\right)$$
Notre chaîne est donc maintenant une chaîne de Markov recurrente positive et l'on peut donc utiliser le théorème ergodique pour montrer que:
$$\frac{1}{n}\sum_{i=1}^n\mathbb{I}_{A_t=T} = \frac{n^T}{n} \underset{n\to+\infty}{\to} C^{ste}$$
ce qui est le premier résultat voulu. Le second résultat découle directement du théorème ergodique en utilisant le fait que $\Delta_T^n$ peut s'écrire comme une somme d'indicatrice. 
\\
\\
Notre famille sera désormais: 
$\tilde \eta_t^{T,\infty} = \left(\text{Imb}_t, \frac{\Delta_t}{\sum_{i=1}^m\mu_i^{\infty}\mathbb{I}_{\text{Imb}_t\in\mathcal{I}^{i}}}\right)=\left(\text{Imb}_t, \tilde{\Delta_t}\right)$
\\
\begin{definition}[Événement rare]
\\
On défini un événement rare une événement à la date $t$ tel que $\tilde{\Delta_t}<\Delta_T(\alpha)$.
    
\end{definition}


Nous allons maintenant énoncer le résultat principal: l'uniformité des événements rares.
\\
\begin{theorem}[Uniformité des événements rares]
\\
\\
Soit un Order book d'un modèle QR de temps $T_f\in \mathbb{R}$, caractérisé par son vecteur d'état $\tilde \eta_{t_i\in [t_1,\dots,T_f]}$.
\\
$\mathbb{P}\left(\tilde\eta_t^T\in q_{\alpha}\right)$ est indépendante du temps quand $n\to\infty$. C'est à dire que les événements rares sont uniforméments repartis dans l'intervalle de temps.

\end{theorem}
\textit{Démonstration} 
\\
On commence par montrer que $\mathbb{P}\left(\tilde\eta_t^{T,\infty}\in q_{\alpha}\right)$ est indépendante du temps quand $n\to+\infty$. La Preuve réside dans la définition de l'événement rare. Vu que $\Delta_T(\alpha)$ ne dépend plus de $n$, on peut écrire cette probabilité comme une somme d'indicatrice ne dépendant que de notre chaîne de Markov. Ainsi, on a nécessairement convergence vers une constante d'après le théorème érgotique. On repasse à $\mathbb{P}\left(\tilde\eta_t^T\in q_{\alpha}\right)$ en utilisant la convergence des $\mu_i^n$.
\\
\\
Ainsi, on a montrer que modèle QR générait de manière uniforme les événements que l'on a caractérisés comme rares. On devrait donc avoir une densité qui tend vers la densité uniforme. 
\\
\\
On énonce maintenant le corollaire que l'on utilisera par la suite. Pour cela on nomme $(S^\gamma_{k})_{k\in \mathbb{N}}$, une sliding window de durée $\gamma$.
\begin{corollary}[Stationnarité de la probabilité d'une Slinding Window] 
\\
\\
Soit $\gamma \in \mathbb{R}$, $(S^\gamma_{k})_{k\in \mathbb{N}}$ une slinding window de durée $\gamma$ et $(\tilde\eta_{t_{i\in \mathbb{N}}})$ notre vecteur d'état. On note $\mathcal{R}(\alpha)$ les évents rares et $e$ les événements:
$$\mathbb{P}\left(e\in S^{\gamma}_k \cap \mathcal{R}(\alpha)\right)\underset{\gamma\to+\infty}{\to}C^{ste}$$
\end{corollary}
\\
\textit{Démonstration} 
\\Il suffit de considérer le QR restreint à chaque $S^{\gamma}_k$. On a donc via le théorème précédent:
$$\mathbb{P}\left(e\in S^{\gamma}_k \cap \mathcal{R}(\alpha)\right)\underset{\gamma\to+\infty}{\to}C^{ste}(k)$$
La non dépendance en fonction du temps assure finalement que $C^{ste}(k) = C^{ste}$
\\
\\
On crée notre test en utilisant une sliding-window $S_t^{\gamma,j}$, où $j$ correspond à la journée considérée:
$$\hat p(S_t^j) = \frac{\#\{e\in \mathcal{R}\cap S_t^{\gamma,j}\}}{\frac{1}{N}\sum_{i=1,i\neq j}^N\#\{e\in S_t^{\gamma,i}\}}$$

\subsection{Hypothèses et attentes théoriques}

    Au vu du modèle Queue Reactive et de la revue de littérature, plusieurs hypothèses et prédictions émergent quant à l'impact des news sur la dynamique du carnet d'ordres.
\\
\\
    En ce qui concerne l'impact sur les intensités de flux d'ordres, le modèle Queue Reactive suggère plusieurs modifications significatives. Nous anticipons une augmentation notable des intensités d'arrivée d'ordres ($\lambda^L$) immédiatement après une news, accompagnée d'une modification des intensités d'annulation ($\lambda^C$) reflétant le repositionnement des acteurs du marché. Les intensités d'ordres de marché ($\lambda^M$) devraient également varier selon la nature de l'information diffusée. Cette dynamique devrait présenter une asymétrie marquée dans les réactions, avec des effets plus prononcés sur les intensités du côté du carnet directement concerné par la news. De plus, nous nous attendons à observer une asymétrie significative entre les news positives et négatives, particulièrement visible dans les intensités d'annulation.
\\
\\
    La structure du carnet d'ordres devrait également subir des modifications substantielles. Nous prévoyons une déformation temporaire des queues, notamment à travers une modification des ratios $q_1/q_{-1}$ reflétant les nouvelles anticipations des acteurs du marché. Cette réorganisation devrait s'étendre aux queues secondaires ($q_{\pm2}$, $q_{\pm3}$) qui s'ajusteront en réaction aux mouvements des meilleures limites. Au niveau de la dynamique des prix, nous anticipons une augmentation de la probabilité de changement de $p_{\text{ref}}$ après une news significative, ainsi qu'une modification du ratio de retour à la moyenne $\eta$ pendant la période d'absorption de l'information.
\\
\\
    Les comportements stratégiques des acteurs devraient suivre un schéma en deux phases distinctes. La phase initiale devrait être caractérisée par une augmentation immédiate des annulations d'ordres limites, accompagnée d'un afflux d'ordres au marché provenant des traders informés cherchant une exécution rapide. Cette phase serait suivie d'une période d'ajustement durant laquelle nous observerions un repositionnement progressif via des ordres limites, ainsi qu'une normalisation graduelle des intensités vers leurs niveaux d'équilibre.
\\
\\
    En termes de liquidité, les effets combinés devraient se manifester selon deux horizons temporels distincts. À court terme, nous prévoyons une détérioration temporaire de la liquidité, caractérisée par une augmentation du spread et une diminution de la profondeur du marché, accompagnée d'une augmentation de la volatilité des prix. À moyen terme, nous anticipons un retour progressif à l'équilibre avec des niveaux de liquidité normalisés, potentiellement accompagné d'une amélioration de l'efficience informationnelle du marché.
\\
\\
    Ces hypothèses constituent le cadre théorique qui guidera notre analyse empirique, permettant ainsi de confronter systématiquement nos prédictions théoriques aux observations concrètes du marché.

    \newpage
    \section{Le modèle QR: observations sur les marchés HF
}
\subsection{Intensité des événements}
Nous avons vu que l'intensité des événements dépendait de la taille de la file d'attente, de la limite considérée et de 
l'action effectuée. Nous allons donc vérifier ce résultat sur nos données. 
\\
\\
Pour cela nous parcourons notre dataframe et nous remplissons un dictionnaire dont la clé est la taille de la liste d'attente. À chaque clé, l'on trouve une liste de 3 tableaux correspondant aux $\Delta_t$ de Add, Trade et Cancel séparés. Nous agrégeons ensuite les résultats par boxplot pour lisser la courbe et diminuer les intervalles d'incertitudes sur les intensités pour chaque boxplot.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{ADD_Cancel.png}
    \caption{Intensité des Add (bleu) et Cancel (vert) en fonction de la taille de queue pour l'actif GOOGL avec intervalle de confiance à 95\% sur la première limite}
    \label{fig:add_cancel}
\end{figure}
\\
Nous avons choisi de représenter les Cancels et les Add ensemble, car ils présentent des caractéristiques similaires, comme remarqué précédemment au niveau de la distribution de l'imbalance. On observe ici que l’intensité semble dans un premier temps augmenter avant d'attendre un palier vers un vingtaine d'actions par seconde. Ainsi, un acteur aura tendance à être globalement indifférent de la taille de queue si celle-ci est suffisamment grande, avec une légère augmentation en fonction de la taille pour les ajouts (add). L'évolution des cancels diffère des courbes trouvées dans la littérature, qui présentent une courbe strictement croissante, avec un début quasi-linéaire, qui est notamment limité par le nombre d'ordres que l'on peut annuler. Ici, le petit tick de l'actif GOOGL et le fait que l'on est sur un marché avec plus d'acteurs HF peuvent expliquer l'évolution observée.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Trades.png}
    \caption{Intensité des Trades en fonction de la taille de queue pour l'actif GOOGL avec intervalle de confiance à 95\% sur la première limite}
    \label{fig:trades}
\end{figure}
\\
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{2ndLim_Intensity.png}
    \caption{Intensité des Add (bleu) et Cancel (vert) en fonction de la taille de queue pour l'actif GOOGL avec intervalle de confiance à 95\% sur la seconde limite}
    \label{fig:trades}
\end{figure}

Pour ce qui est des trades, on remarque un comportement bien différent. L'intensité augmente drastiquement lorsque la queue est très petite, ce qui correspond globalement aux moments où l'imbalance est le plus grand en valeur absolue. On retrouve donc ici une certaine cohérence avec la densité de l'imbalance des trades. Par ailleurs, l'intensité semble décroître exponentiellement vers une limite par la suite, qui correspond au fait que l'acteur devient de plus en plus indifférent à la taille de queue, avant de remonter quand la taille devient très grande (l'imbalance tend vers 1 en valeur absolue). En effet, un acteur à tout intérêt à consommer de la liquidité 'gratuite', qui correspond aux tailles de queues importantes. 
\\
\\
L'étude sur la seconde limite montre un comportement similaire sur les add et les cancel mais avec ici une intensité plus faible. On a donc tendance à avoir moins d'action sur la seconde limite que sur la première pour cette actif.
\\
\\
Ces résultats étant déjà obtenus dans la littérature (voir C-A.Lehalle, Mathieu Rosenbaum et Weibing Huang, \textit{Simulating and analyzing order book data:
The queue-reactive model}) nous nous intéressons par la suite au tracé des intensités en fonction de l'imbalance. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{trades_imbalnces_intensity.png}
    \caption{Intensité des Trades en fonction de l'imbalance pour l'actif GOOGL avec intervalle de confiance à 95\% sur la première limite}
    \label{fig:trades}
\end{figure}
\\
Pour commencer on observe bien la symétrie de l'imbalance que l'on avait pour les densités. Le fait de ne pas séparer les ask et les bid est donc ici partiellement vérifié ici. Nous observons ici que plus la valeur absolue de l'imbalance est importante plus un acteur aura tendance à réaliser un trade rapidement.  Comme nous l'avons vu précédemment, les trades on plus souvent lieu aux imbalances extrêmes et l'on peut désormais rajouter qu'ils sont réalisé avec une plus grande intensité. Les acteurs cherchent donc à prendre avantage du MBO en se plaçant dans des imbalances extrêmes et en réagissant plus vite dans ces cas, vu que la probabilité d'un delta de prix de même signe que l'imbalance devient plus grande comme remarqué précédemment. Afin de vérifier qu'il s'agit d'une observation non propre à notre actif, on trace ci-dessous le même graphe pour KHC (plus gros tick) et LCID (gros tick prix faible).
\begin{figure}[h!]
    \centering
    % Première figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ImbalancefcttradesKHC.png}
        \caption{KHC}1
        \label{fig:imbalance}
    \end{subfigure}
    \hfill
    % Deuxième figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{KHC_Trades_ints_imb.png}
        \caption{LCID}
        \label{fig:densite_imbalance}
    \end{subfigure}
    \caption{Intensité des Trades en fonction de l'imbalance pour l'actif KHC (gauche) et LCID (droite) avec intervalle de confiance à 95\% sur la première limite}
    \label{fig:comparison}
\end{figure}
\\
\\
On voit bien un comportement similaire, qui semble tout de même plus marqué plus le tick est gros, par exemple pour LCID, gros tick prix faible, qui fit donc mieux le QR, on voit que l'intensité augmente énormément lorsque l'on s'approche des bornes de l'imbalance, ce qui signifie que les acteurs deviennent beaucoup plus réactifs proche des imbalances extrêmes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{ADD_CANCEL_INTENSITY_IMBALANCE_GOOGL.png}
    \caption{Intensité des Add (bleue) et Cancel (vert) en fonction de l'imbalance pour l'actif GOOGL avec intervalle de confiance à 95\% sur la seconde limite}
    \label{fig:trades}
\end{figure}
\\
\\
Pour ce qui est des add et des cancel, l'observation est moins flagrante et l'on tend plus vers une intensité qui reste constante quelque soit l'imabalance, mis à part pour des imbalance extrêmes, où les ajouts semblent être plus rapide puis moins rapide pour des imbalances proche de -1, qui correspondent au cas où la quantité d'ask est très grande devant le bid. On remarque cependant que l'intensité augmente pour les add, proche de l'imbalance 1, ce qui casse la symétrie bid-ask précédement observé. Les acteurs auraient une préférence pour l'offre sur cette actif, ce qui notamment expliquerait l'augmentation globale du prix de l'actif sur la durée d'étude. On peut tout de mêmes tracer les même graphes sur les autres actifs, qui sont plus concluants et qui conservent mieux la symétrie.
\begin{figure}[h!]
    \centering
    % Première figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imbalnce fctIntensitéADD:Cancel.png}
        \caption{KHC}
        \label{fig:imbalance}
    \end{subfigure}
    \hfill
    % Deuxième figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ADD_CANCEL_INTENSITY_IMBALNCE_LCID.png}
        \caption{LCID}
        \label{fig:densite_imbalance}
    \end{subfigure}
    \caption{Intensité des Add (bleu) et des Cancel (vert) en fonction de l'imbalance pour l'actif KHC (gauche) et LCID (droite) avec intervalle de confiance à 95\% sur la première limite}
    \label{fig:comparison}
\end{figure}
Là encore, les résultats sont plus concluants: on a un comportement plus prononcé sur les extrémités, avec une augmentation de l'intensité comme vue sur les trades précédemment.
\\
\\
L'observation sur les limites supérieures donne des résultats similaires à ceux de la première limite, avec des comportements moins prononcés que pour la première limite.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{DErnierGraph_LCID_INTENSITYxImbalance.png}
    \caption{Intensité des Add (bleue) et Cancel (vert) en fonction de l'imbalance pour l'actif LCID avec intervalle de confiance à 95\% sur la seconde limite}
    \label{fig:trades}
\end{figure}


\subsection{Densité des News}
Nous allons désormais nous intéresser à la localisation des événements caractérisés comme rares. On caractérise la rareté d'un événement par sa grande intensité. Plus un événement à une intensité faible, plus il sera rare. Ainsi dans le modèle QR, nous avons précédemment montré que ces événements étaient localisés uniformément sous l'hypothèse du modèle QR vraie. Afin de ne pas être biaisé par les effets d'ouverture et de fermeture des marchés, nous nous plaçons à une heure de chaque limite. Nous nous plaçons sur une seule journée, où nous prenons les événements les 5\% les plus rares. Ceux-ci sont donc théoriquement localisés uniformément sur la journée.
\\
\\
Précisons la caractéristique d'événement rares. On commence par regrouper les événements par imbalance. Nous calculons ensuite l'intensité de chaque événement dans chaque bucket d'imbalance normalisée par bucket d'imbalance. Après avoir calculé cela, 
nous avons une série de valeurs normalisées et l'on peut enlever la dépendance à l'imbalance. On sélectionne donc parmi ces valeurs les 5\% les plus faibles, qui sont donc les plus rares au sens du QR. On note cet ensemble $\mathcal{R}$. Afin de calculer la probabilité $p(t)$ d'un événement rare, nous parcourons tout nos événements avec une sliding window de 5 minutes et nous calculons donc la probabilité d'un événement rare sur la sliding window de 5 minutes au jour $j$ noté $S_t^j$:
\\
$$\hat{p}(S_t^j) = \frac{\#\{e\in \mathcal{R}\cap S_t^j\}}{\frac{1}{N}\sum_{i=1,i\neq j}^N\#\{e\in S_t^i\}}$$
\\
On devrait donc avoir une droite proche de $5\%$ tout les jours. Cependant, après quelques tracés seulement on observe que cette probabilité n'est pas du tout uniforme certain jour.

Pour une étude plus approfondie, nous sélectionnons les jours $j$ où les probabilité $p_{max} = \underset{t\in \mathcal{T}}{\max}\ \hat{p}(S_t^j)$ sont les plus importantes.  Nous présentons ici les 6 jours avec la probabilité la plus importante.

 \\
\\
\\
\\
\\
\begin{figure}[h!]
    \centering
    % Premier graphique
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe3.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 10 novembre 2024.}
        \label{fig:graphe3}
    \end{subfigure}
    \hfill
    % Deuxième graphique
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe3Prix.png}
        \caption{Prix de l'actif GOOGL le 10 novembre 2024.}
        \label{fig:graphe3Prix}
    \end{subfigure}

    \caption{Analyse de l'actif GOOGL le 10 novembre 2024 : $\hat{p}(S_t^j)$ et évolution des prix.}
    \label{fig:comparison}
\end{figure}
\\

\begin{figure}[h!]
    \centering
    % Premier graphique
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe_1.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 20 septembre 2024.}
        \label{fig:graphe1}
    \end{subfigure}
    \hfill
    % Deuxième graphique
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe_1_prix.png}
        \caption{Prix de l'actif GOOGL le 20 septembre  2024.}
        \label{fig:graphe2}
    \end{subfigure}

    \caption{Analyse de l'actif GOOGL le 20 septembre  2024 : $\hat{p}(S_t^j)$ et évolution des prix.}
    \label{fig:side_by_side}
\end{figure}

\clearpage
\\
\\
% Première paire d'événements
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe_1.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 20 septembre 2024: Plainte collective au Royaume-Uni pour pratiques monopolistiques, risque de perte de consommateur importante.}
        \label{fig:trades1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe4.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 25 juillet 2024: Publication des résultats financiers du deuxième trimestre 2024, suivie de l'annonce d'un nouveau modèle pour ChatGPT.}
        \label{fig:trades2}
    \end{subfigure}
    \caption{Événements majeurs impactant GOOGL (1/3)}
    \label{fig:comparison1}
\end{figure}

% Deuxième paire d'événements
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe5.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 17 novembre 2024: Lancement de l'IA Gemini, Présentation des smartphones Pixel 9, Amende record en Russie.}
        \label{fig:trades3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe6.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 29 août 2024: Annonce d'une brèche de sécurité dans Google Chrome.}
        \label{fig:trades4}
    \end{subfigure}
    \caption{Événements majeurs impactant GOOGL (2/3)}
    \label{fig:comparison2}
\end{figure}


% Troisième paire d'événements
\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphe7.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 18 septembre 2024: Annonce du Cut des taux de FED.}
        \label{fig:trades5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.40\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Grpahe7.png}
        \caption{$\hat{p}(S_t^j)$ de GOOGL le 5 août 2024: Perte du Procès contre la justice américaine pour pratiques anticoncurrentielles, amende importante.}
        \label{fig:trades6}
    \end{subfigure}
    \caption{Événements majeurs impactant GOOGL (3/3)}
    \label{fig:comparison3}
\end{figure}

\\

Les graphes de probabilité sont en effet très différents: d'un côté la probabilité oscille autour de 0.05, de l'autre elle possède un net pic à 18:00:00 (heure française des marchés américain). La probabilité n'est pas uniforme dans le second cas. Cela se voit directement sur le graphe des prix, qui semble relativement normale pour la première journée, là où il possède un pic à $18:00:00$ le 20 septembre. On en conclut donc que les évènements extrêmes ne sont pas uniformes (tout les jours) et ne sont pas toujours causés par le MBO. On semble tout de même avoir différents régimes comme vu précédemment.
\\

Il est alors facile de comprendre ce qu'il se passe en réalité: ces jours correspondent à l'annonce de news importantes concernant Google. Nous avons donc une corrélation directe entre événements rares et news extérieurs. \textbf{Il n'est donc pas possible de capter l'intégralité des mouvements de prix dans le MBO!} Il y a par conséquent une nécessité d'apporter des facteurs exogènes qui viennent incorporer ces événements dans la modélisation de l'order book.

\\ 
\clearpage








\section*{Appendice}
\addcontentsline{toc}{section}{Appendice}
\section*{Test statistique sur sliding window temporelle}

Dans cette section, nous construisons un test statistique basé sur les développements précédents afin de déterminer l'uniformité des événements rares dans le carnet d'ordres. Nous présenterons à la fois des intervalles de confiance asymptotiques et non asymptotiques.

\subsubsection*{Hypothèses du test}

Nous considérons les hypothèses suivantes :

\begin{itemize}
    \item \textbf{Hypothèse nulle ($H_0$)} : Les événements rares sont distribués uniformément dans le temps.
    \item \textbf{Hypothèse alternative ($H_A$)} : Les événements rares ne sont pas distribués uniformément, indiquant une agrégation temporelle.
\end{itemize}

\subsubsection*{Définition de la statistique de test}

Nous définissons la statistique de test basée sur la proportion d'événements rares dans une fenêtre glissante de durée $\gamma$ :

$$
\hat{p}(S_k^{\gamma}) = \frac{\#\{e \in \mathcal{R} \cap S_k^{\gamma}\}}{\frac{1}{N} \sum_{i=1, i \neq j}^N \#\{e \in S_k^{\gamma, i}\}}
$$

où :
\begin{itemize}
    \item $S_k^{\gamma}$ est une fenêtre glissante de durée $\gamma$.
    \item $\mathcal{R}$ est l'ensemble des événements rares.
    \item $N$ est le nombre total de fenêtres considérées.
\end{itemize}

\subsubsection*{Construction de l'intervalle de confiance asymptotique}

Sous l'hypothèse $H_0$, grâce au théorème ergodique, la proportion $\hat{p}(S_k^{\gamma})$ converge vers une constante $C^{ste}$ lorsque $n \to \infty$. Pour les chaînes de Markov, le Théorème Central Limite \cite{jones2005markov} nécessite des conditions supplémentaires :

\begin{itemize}
    \item La chaîne doit être irréductible et apériodique
    \item La chaîne doit être réversible
    \item La variance asymptotique $\sigma^2_{\infty}$ doit être finie
\end{itemize}

Sous ces conditions, nous pouvons approximer la distribution asymptotique de $\hat{p}(S_k^{\gamma})$ par :

$$
\sqrt{n}(\hat{p}(S_k^{\gamma}) - C^{ste}) \overset{d}{\to} \mathcal{N}(0, \sigma^2_{\infty})
$$

où $\sigma^2_{\infty}$ est la variance asymptotique qui dépend de la structure d'autocorrélation de la chaîne :

$$
\sigma^2_{\infty} = \gamma(0) + 2\sum_{k=1}^{\infty}\gamma(k)
$$

avec $\gamma(k)$ l'autocovariance d'ordre $k$ de la chaîne.

Ainsi, un intervalle de confiance asymptotique à $1 - \alpha$ niveau de confiance est donné par :

$$
IC_{1 - \alpha}^{\text{asymptotique}} = \left[ \hat{p}(S_k^{\gamma}) - z_{1 - \alpha/2} \frac{\hat{\sigma}_{\infty}}{\sqrt{n}}, \ \hat{p}(S_k^{\gamma}) + z_{1 - \alpha/2} \frac{\hat{\sigma}_{\infty}}{\sqrt{n}} \right]
$$

où $\hat{\sigma}_{\infty}$ est un estimateur consistant de $\sigma_{\infty}$ qui peut être obtenu par exemple via la méthode des batchmeans ou par estimation spectrale \cite{flegal2010batch}.

\subsubsection*{Construction de l'intervalle de confiance non asymptotique}

Pour les intervalles de confiance non asymptotiques, nous utilisons l'inégalité de Hoeffding, qui fournit une borne sur la probabilité que la proportion observée s'écarte de la proportion attendue indépendamment de la taille de l'échantillon. Sous $H_0$, la probabilité que la proportion réelle diffère de la proportion observée par plus de $\epsilon$ est bornée par :

$$
\mathbb{P}\left( \left| \hat{p}(S_k^{\gamma}) - C^{ste} \right| \geq \epsilon \right) \leq 2 \exp(-2 n \gamma^2 \epsilon^2)
$$

Pour obtenir un intervalle de confiance non asymptotique à un niveau de confiance $1 - \alpha$, nous résolvons l'inégalité suivante :

$$
2 \exp(-2 n \gamma^2 \epsilon^2) \leq \alpha
$$

Ce qui implique :

$$
\epsilon = \sqrt{\frac{\ln(2/\alpha)}{2 n \gamma^2}}
$$

Ainsi, l'intervalle de confiance non asymptotique est donné par :

$$
IC_{1 - \alpha}^{\text{non-asymptotique}} = \left[ \hat{p}(S_k^{\gamma}) - \sqrt{\frac{\ln(2/\alpha)}{2 n \gamma^2}}, \ \hat{p}(S_k^{\gamma}) + \sqrt{\frac{\ln(2/\alpha)}{2 n \gamma^2}} \right]
$$



\newpage 
\subsection*{Test statistique sur sliding window évènementielle finie}

    Considérons $Q$ processus de Poisson indépendants $(T_i)_{i=1,\ldots,Q}$ d'intensités respectives $\lambda_i$ qui représentent les différents temps d'arrivées pour chaque Market Order de taille $i$ conditionnellement à la forme du carnet d'ordre. Soit $(S_n)_{n\geq 1}$ la suite des temps de saut du processus minimum $\min_{i=1,\ldots,Q} T_i$. On note $\Delta_n = S_n - S_{n-1}$ les inter-arrivées.
\\

Nous disposons ainsi d'une matrice d'intensités, où les lignes représentent les différents buckets d'imbalance et les colonnes correspondent aux $Q$ processus de Poisson. Cette structuration matricielle est fondamentale dans le modèle QR, car elle intègre la connaissance du carnet d'ordres avec l'imbalance, permettant une modélisation précise des dynamiques du marché.
\\

    Pour chaque bucket d'imbalance (par exemple, -1, -0.9, ..., 1), les intensités $\lambda_i$ sont conditionnées à l'état du carnet d'ordres, garantissant que les $\Delta_n$ sont distribuées selon une loi exponentielle après normalisation. En utilisant le test de Kolmogorov-Smirnov, nous pouvons inverser la fonction de répartition avec une précision de l'ordre de $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$.

\\

    Ainsi, bien que nous considérions $Q$ processus indépendants, la normalisation assure que la distribution des $\Delta_n$ reste cohérente avec les hypothèses de notre modèle, permettant une approximation binomiale valide pour le nombre d'événements rares dans chaque fenêtre glissante.
    \\
    
    \begin{proposition}
    \\
    
    Pour un quantile $\alpha \in (0,1)$ et une fenêtre glissante de taille $N$, sous l'hypothèse de distribution uniforme des événements rares (définis comme les $\Delta_n$ inférieurs au quantile $\alpha$), le nombre $X_k$ d'événements rares dans la $k$-ième fenêtre suit une loi binomiale :

    \[
    X_k \sim \mathcal{B}(N, \alpha)
    \]
    \end{proposition}

    \begin{proof}
    \\

    La distribution des inter-arrivées $\Delta_n$ du processus minimum suit une loi exponentielle d'intensité $\Lambda = \sum_{i=1}^Q \lambda_i$. La fonction de densité est donnée par :

    \[
    f_{\Delta}(\delta) = \Lambda e^{-\Lambda \delta} \quad \text{pour } \delta \geq 0
    \]

    Considérons un quantile $\alpha \in (0,1)$. Le seuil $s_\alpha$ est défini tel que :

    \[
    P(\Delta_n \leq s_\alpha) = \alpha \quad \Rightarrow \quad s_\alpha = F^{-1}(\alpha) = -\frac{\ln(1 - \alpha)}{\Lambda}
    \]

    où $F$ est la fonction de répartition de la loi exponentielle. Cette normalisation permet de récupérer une distribution empirique des temps entre sauts ($\Delta_n$) dans chaque bucket d'imbalance, en les normalisant par la moyenne des $\Delta_n$, obtenant ainsi la loi exponentielle, conformément au TLC.



    Pour une fenêtre glissante de taille $N$, chaque position dans la fenêtre est indépendante des autres étant donné que les $\Delta_n$ sont indépendantes et identiquement distribuées (i.i.d.). La probabilité qu'un inter-arrivée $\Delta_n$ soit un événement rare est donc constante et égale à $\alpha$.

    Le nombre $X_k$ d'événements rares dans la $k$-ième fenêtre est la somme de $N$ variables indicatrices indépendantes, chacune prenant la valeur 1 avec probabilité $\alpha$ et 0 sinon. Ainsi :

    \[
    X_k = \sum_{i=1}^{N} \mathbb{I}_{\{\Delta_i \leq s_\alpha\}}
    \]

    Par définition, $X_k$ suit une loi binomiale de paramètres $N$ et $\alpha$ :

    \[
    X_k \sim \mathcal{B}(N, \alpha)
    \]
    
    \end{proof}

    \newline
    L'approximation binomiale est justifiée par l'indépendance des événements rares dans la fenêtre glissante. De plus, pour assurer une approximation précise, il est nécessaire que $N$ soit suffisamment grand et que $\alpha$ soit petit, ce qui garantit que la variance $\alpha(1 - \alpha)$ est bien estimée.
\\
    Par conséquent, pour un niveau de confiance $1 - \gamma$, l'intervalle de confiance pour la proportion $p_k = \frac{X_k}{N}$ peut être quantifié avec une précision accrue en utilisant une correction de continuité comme l'intervalle de Wilson :

    \[
    IC_{\text{Wilson}} = \left[ \frac{\hat{p} + \frac{z^2}{2N} - z \sqrt{ \frac{\hat{p}(1 - \hat{p})}{N} + \frac{z^2}{4N^2} }}{ 1 + \frac{z^2}{N} }, \frac{\hat{p} + \frac{z^2}{2N} + z \sqrt{ \frac{\hat{p}(1 - \hat{p})}{N} + \frac{z^2}{4N^2} }}{ 1 + \frac{z^2}{N} } \right]
    \]

    où $\hat{p} = \frac{X_k}{N}$ et $z$ est le quantile de la loi normale standard correspondant au niveau de confiance choisi.

\\

    \textbf{Formulation des tests statistiques :}

    Étant donné que la taille de l'échantillon $N$ est d'environ 10, l'utilisation de l'intervalle de Wilson est particulièrement appropriée pour estimer la proportion $p_k$. Pour N grand, des intervalles de confiance classiques tels que le classique intervalle de Wald peuvent être employés, bien que moins précis pour de petites tailles d'échantillons. 
    \[
    IC_{\text{Wald}} = \left[ \hat{p} - z \sqrt{ \frac{\hat{p}(1 - \hat{p})}{N} }, \hat{p} + z \sqrt{ \frac{\hat{p}(1 - \hat{p})}{N} } \right]
    \]

    Cependant, l'intervalle de Wald est moins précis pour de petites tailles d'échantillons et des proportions proches de 0 ou 1, ce qui est notre cas ici avec $\hat{p} = 0.05$.



    \newpage
\section*{Conclusion}

Ce mémoire apporte plusieurs contributions significatives à la compréhension des dynamiques de marché à haute fréquence et particulièrement à l'étude de l'impact des news sur les Limit Order Books.

\paragraph{\textbf{Notre première contribution}} est d'ordre méthodologique. En développant un cadre probabiliste et statistique rigoureux, nous avons pu identifier et localiser précisément les moments où la dynamique du carnet d'ordres s'écarte significativement du modèle Queue-Reactive classique. Cette approche, basée sur l'étude des événements caractérisé comme rare par le modèle permet donc une détection fine des anomalies dans le flux d'ordres.

\paragraph{\textbf{Notre seconde contribution}} est la mise en évidence empirique des limites du modèle Queue-Reactive standard. En effet, l'analyse des données de marché pour les actifs GOOGL, LCID et KHC révèle que certains mouvements de prix significatifs ne peuvent être expliqués uniquement par la microstructure endogène du carnet d'ordres. Ces moments correspondent précisément à l'arrivée d'informations exogènes importantes (résultats financiers, décisions réglementaires, annonces stratégiques) qui impactent brutalement la formation des prix.

\paragraph{} Cette observation fondamentale suggère qu'une modélisation complète de la dynamique des prix en haute fréquence nécessite l'intégration de deux composantes :
\begin{itemize}
    \item Une composante endogène, bien capturée par le modèle Queue-Reactive, qui décrit les interactions entre offre et demande dans le carnet d'ordres
    \item Une composante exogène, liée à l'arrivée d'informations externes, qui provoque des changements de régimes temporaires
\end{itemize}

\paragraph{}Des outils statistiques plus complets de détection par vraisemblance ou paramétrisation des probabilités de sauts $\theta$ pourraient être utilisés en temps réel pour identifier les instants où l'information exogène domine la dynamique de marché et où l'on aurait un changement de régime régulier et chaotiques directement inclus dans le modèle afin d'améliorer les stratégies de trading algorithmique ou de market-making en période incertaine.



    
    \newpage
    \addcontentsline{toc}{section}{Bibliography}
    \begin{thebibliography}{9}

    \bibitem{lehalle2021optimal}
    Charles-Albert Lehalle, Othmane Mounjid, Mathieu Rosenbaum (2021)
    \textit{Optimal Liquidity-Based Trading Tactics},
    Stochastic Systems 11(4):368-390

    \bibitem{lehalle2018limit}
    Charles-Albert Lehalle, and Othmane Mounjid (2018)
    \textit{Limit Order Strategic Placement with Adverse Selection Risk and the Role of Latency},
    CFM

    \bibitem{lehalle2014simulating}
    Charles-Albert Lehalle, Mathieu Rosenbaum and Weibing Huang (2014)
    \textit{Simulating and analyzing order book data: The queue-reactive model},
    CFM

    \bibitem{jones2005markov}
    Jones, Galin L. (2005)
    \textit{On the Markov chain central limit theorem},
    arXiv:math/0409112

    \bibitem{flegal2010batch}
Flegal, J. M., & Jones, G. L. (2010)
\textit{Batch means and spectral variance estimators in Markov chain Monte Carlo},
The Annals of Statistics, 38(2), 1034-1070


    \end{thebibliography}
\end{document}