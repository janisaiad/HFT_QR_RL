{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # ca on va devoir bcp l'uiliser\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, depth):\n",
    "    df = df.replace(614, 0)\n",
    "    df = df[df['symbol'] == 'GOOGL']\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    df[f'bid_sz_0{depth}_diff'] = df[f'bid_sz_0{depth}'].diff()\n",
    "    df[f'ask_sz_0{depth}_diff'] = df[f'ask_sz_0{depth}'].diff()\n",
    "    df = df[df['depth'] == depth]\n",
    "    # df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    # try:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='coerce')\n",
    "    # except ValueError:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df['temps_ecoule'] = df['ts_event'].diff()\n",
    "    df['temps_ecoule_secondes'] = df['temps_ecoule'].dt.total_seconds()\n",
    "    condition_T = (\n",
    "        (df['action'] == 'T') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_A = (\n",
    "        (df['action'] == 'A') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_C = (\n",
    "        (df['action'] == 'C') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    df['status'] = np.where(condition_T | condition_A | condition_C, 'OK', 'NOK')\n",
    "    df = df[df['status'] == 'OK']\n",
    "    return df\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "files_csv = glob.glob(os.path.join(\"/Volumes/T9/CSV_dezippe_nasdaq\", \"*.csv\"))\n",
    "#files_csv = glob.glob(os.path.join(\"/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_ct_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_ct_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_ct_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_ct_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq/xnas-itch-20240723.mbp-10.csv')\n",
    "# df = df[df['symbol'] == 'GOOGL']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 100, threshold=1000)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 1000\n",
    "\n",
    "#quarter = []\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            #quarter.append(np.var(tab)*1/np.mean(tab)*len(intensities[i][0])/len(tab)*1/np.sqrt(len(tab)))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = np.array(Add)+1.96*np.array(quarter), mode ='lines', name ='Add', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = np.array(Add)-1.96*np.array(quarter), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_ct_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_ct_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_ct_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_ct_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'GOOGL']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=50)\n",
    "\n",
    "\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, depth):\n",
    "    df = df.replace(614, 0)\n",
    "    df = df[df['symbol'] == 'KHC']\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    df[f'bid_sz_0{depth}_diff'] = df[f'bid_sz_0{depth}'].diff()\n",
    "    df[f'ask_sz_0{depth}_diff'] = df[f'ask_sz_0{depth}'].diff()\n",
    "    df = df[df['depth'] == depth]\n",
    "    # df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    # try:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='coerce')\n",
    "    # except ValueError:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df['temps_ecoule'] = df['ts_event'].diff()\n",
    "    df['temps_ecoule_secondes'] = df['temps_ecoule'].dt.total_seconds()\n",
    "    condition_T = (\n",
    "        (df['action'] == 'T') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_A = (\n",
    "        (df['action'] == 'A') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_C = (\n",
    "        (df['action'] == 'C') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    df['status'] = np.where(condition_T | condition_A | condition_C, 'OK', 'NOK')\n",
    "    df = df[df['status'] == 'OK']\n",
    "    return df\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "files_csv = glob.glob(os.path.join(\"/Volumes/T9/CSV_dezippe_nasdaq\", \"*.csv\"))\n",
    "#files_csv = glob.glob(os.path.join(\"/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'KHC']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities KHC premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'KHC']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=500)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities KHC seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, depth):\n",
    "    df = df.replace(614, 0)\n",
    "    df = df[df['symbol'] == 'LCID']\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    df[f'bid_sz_0{depth}_diff'] = df[f'bid_sz_0{depth}'].diff()\n",
    "    df[f'ask_sz_0{depth}_diff'] = df[f'ask_sz_0{depth}'].diff()\n",
    "    df = df[df['depth'] == depth]\n",
    "    # df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    # try:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='coerce')\n",
    "    # except ValueError:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df['temps_ecoule'] = df['ts_event'].diff()\n",
    "    df['temps_ecoule_secondes'] = df['temps_ecoule'].dt.total_seconds()\n",
    "    condition_T = (\n",
    "        (df['action'] == 'T') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_A = (\n",
    "        (df['action'] == 'A') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_C = (\n",
    "        (df['action'] == 'C') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    df['status'] = np.where(condition_T | condition_A | condition_C, 'OK', 'NOK')\n",
    "    df = df[df['status'] == 'OK']\n",
    "    return df\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "files_csv = glob.glob(os.path.join(\"/Volumes/T9/CSV_dezippe_nasdaq\", \"*.csv\"))\n",
    "#files_csv = glob.glob(os.path.join(\"/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'LCID']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities LUCID premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'LCID']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 100\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities LUCID seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
