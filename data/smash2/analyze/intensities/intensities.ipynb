{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import cupy as cp\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les données JSON\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Charger les fichiers JSON\n",
    "condition = load_json('/users/eleves-a/2022/janis.aiad/3A/EAP1/HFT_QR_RL/HFT_QR_RL/data/smash2/data/dbn/condition.json')\n",
    "manifest = load_json('/users/eleves-a/2022/janis.aiad/3A/EAP1/HFT_QR_RL/HFT_QR_RL/data/smash2/data/dbn/manifest.json')\n",
    "metadata = load_json('/users/eleves-a/2022/janis.aiad/3A/EAP1/HFT_QR_RL/HFT_QR_RL/data/smash2/data/csv/metadata.json')\n",
    "\n",
    "# Fonction pour charger les données CSV\n",
    "def load_csv(stock, date):\n",
    "    file_path = f'/users/eleves-a/2022/janis.aiad/3A/EAP1/HFT_QR_RL/HFT_QR_RL/data/smash2/data/csv/{stock}/{date}.csv'\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Spécifier les dates et les stocks\n",
    "dates = [\"20240624\", \"20240625\", \"20240626\", \"20240627\", \"20240628\", \"20240701\", \"20240702\", \"20240703\", \"20240705\", \"20240708\", \"20240709\", \"20240710\", \"20240711\", \"20240712\", \"20240715\", \"20240716\", \"20240717\", \"20240718\", \"20240719\", \"20240722\", \"20240723\", \"20240724\", \"20240725\", \"20240726\", \"20240729\", \"20240730\", \"20240731\", \"20240801\", \"20240802\", \"20240805\", \"20240806\", \"20240807\", \"20240808\"]\n",
    "stocks = [\"HL\",'ASAI','RIOT','CGAU']\n",
    "# Charger les données pour chaque stock et chaque date dans des datasets différents\n",
    "data_dict = {}\n",
    "for stock in stocks:\n",
    "    data_dict[stock] = {}\n",
    "    for date in dates:\n",
    "        data_dict[stock][date] = load_csv(stock, date).sample(frac=0.1, random_state=1)\n",
    "# Concaténer toutes les données\n",
    "data_list = [data_dict[stock][date] for stock in stocks for date in dates]\n",
    "data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Filtrer par publisher_id = 39\n",
    "data = data[data['publisher_id'] == 39]\n",
    "\n",
    "# Convertir ts_event en datetime\n",
    "data['ts_event'] = pd.to_datetime(data['ts_event'], utc=True)\n",
    "data = data.sort_values(by='ts_event')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intensités pour HL:\n",
      "  Date: 20240624\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'20240624'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m dates:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Annulations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintensities[stock][date][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcancel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Ajouts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintensities[stock][date][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Ordres de marché: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintensities[stock][date][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarket_order\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '20240624'"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Calcul des intensités de Poisson pour chaque stock et chaque jour\n",
    "def calculate_poisson_intensity(events, total_time):\n",
    "    return len(events) / total_time\n",
    "\n",
    "# Dictionnaire pour stocker les intensités\n",
    "intensities = {stock: {} for stock in stocks}\n",
    "\n",
    "def process_stock_date(stock, date):\n",
    "    stock_data = data_dict[stock][date]\n",
    "    \n",
    "    # Calculer la durée totale de la journée en secondes\n",
    "    start_time = pd.to_datetime(stock_data['ts_event'].min(), utc=True)\n",
    "    end_time = pd.to_datetime(stock_data['ts_event'].max(), utc=True)\n",
    "    total_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # Initialiser les dictionnaires pour stocker les événements\n",
    "    cancel_events = {i: [] for i in range(1, 6)}  # Pour les 5 meilleurs niveaux\n",
    "    add_events = {i: [] for i in range(1, 6)}\n",
    "    market_order_events = []\n",
    "    \n",
    "    # Parcourir les données pour collecter les événements\n",
    "    for _, row in stock_data.iterrows():\n",
    "        action = row['action']\n",
    "        side = row['side']\n",
    "        depth = row['depth']\n",
    "        \n",
    "        if action == 'Cancel' and side == 'Bid' and depth <= 5:  # Cancel bid\n",
    "            cancel_events[depth].append(row['ts_event'])\n",
    "        elif action == 'Add' and side == 'Bid' and depth <= 5:  # Add bid\n",
    "            add_events[depth].append(row['ts_event'])\n",
    "        elif action == 'Trade' and side == 'Ask':  # Trade ask (market order)\n",
    "            market_order_events.append(row['ts_event'])\n",
    "    \n",
    "    # Calculer les intensités pour chaque type d'événement\n",
    "    intensities[stock][date] = {\n",
    "        'cancel': {level: calculate_poisson_intensity(events, total_time) for level, events in cancel_events.items()},\n",
    "        'add': {level: calculate_poisson_intensity(events, total_time) for level, events in add_events.items()},\n",
    "        'market_order': calculate_poisson_intensity(market_order_events, total_time)\n",
    "    }\n",
    "\n",
    "# Utiliser joblib pour paralléliser le traitement\n",
    "Parallel(n_jobs=-1)(delayed(process_stock_date)(stock, date) for stock in stocks for date in dates)\n",
    "\n",
    "# Afficher un résumé des intensités calculées\n",
    "for stock in stocks:\n",
    "    print(f\"\\nIntensités pour {stock}:\")\n",
    "    for date in dates:\n",
    "        print(f\"  Date: {date}\")\n",
    "        print(f\"    Annulations: {intensities[stock][date]['cancel']}\")\n",
    "        print(f\"    Ajouts: {intensities[stock][date]['add']}\")\n",
    "        print(f\"    Ordres de marché: {intensities[stock][date]['market_order']}\")\n",
    "\n",
    "# Fonction pour calculer l'intensité en fonction de la taille de la file d'attente\n",
    "def queue_size_intensity(stock_data, action, side='Bid'):\n",
    "    queue_sizes = stock_data[stock_data['side'] == side]['size']\n",
    "    events = stock_data[(stock_data['action'] == action) & (stock_data['side'] == side)]\n",
    "    \n",
    "    # Vérifier si les événements ne sont pas vides\n",
    "    if events.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    \n",
    "    # Grouper les événements par taille de file d'attente\n",
    "    grouped_events = events.groupby(pd.cut(events['size'], bins=10))\n",
    "    \n",
    "    # Calculer l'intensité pour chaque groupe\n",
    "    intensities = grouped_events.size() / len(stock_data)\n",
    "    return intensities\n",
    "\n",
    "# Calculer les intensités en fonction de la taille de la file d'attente pour chaque stock et date\n",
    "queue_size_intensities = {stock: {} for stock in stocks}\n",
    "\n",
    "def process_queue_size_intensity(stock, date):\n",
    "    stock_data = data_dict[stock][date]\n",
    "    queue_size_intensities[stock][date] = {\n",
    "        'cancel': queue_size_intensity(stock_data, 'Cancel'),\n",
    "        'add': queue_size_intensity(stock_data, 'Add'),\n",
    "        'market_order': queue_size_intensity(stock_data, 'Trade', 'Ask')\n",
    "    }\n",
    "\n",
    "# Utiliser joblib pour paralléliser le traitement\n",
    "Parallel(n_jobs=-1)(delayed(process_queue_size_intensity)(stock, date) for stock in stocks for date in dates)\n",
    "\n",
    "# Afficher un résumé des intensités en fonction de la taille de la file d'attente\n",
    "for stock in stocks:\n",
    "    print(f\"\\nIntensités en fonction de la taille de la file d'attente pour {stock}:\")\n",
    "    for date in dates:\n",
    "        print(f\"  Date: {date}\")\n",
    "        print(f\"    Annulations: {queue_size_intensities[stock][date]['cancel']}\")\n",
    "        print(f\"    Ajouts: {queue_size_intensities[stock][date]['add']}\")\n",
    "        print(f\"    Ordres de marché: {queue_size_intensities[stock][date]['market_order']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
