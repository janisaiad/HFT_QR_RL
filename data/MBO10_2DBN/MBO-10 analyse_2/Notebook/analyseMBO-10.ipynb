{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # ca on va devoir bcp l'uiliser\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', None) # pour \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(store, level):\n",
    "    # Initialize empty list to store filtered data\n",
    "    filtered_data = []\n",
    "    \n",
    "    # Get unique symbols from metadata \n",
    "    symbols = store.metadata.symbols\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        # Get data for this symbol\n",
    "        symbol_data = store.get_range(\n",
    "            stype_in=['mbp-1'],  # Market by price level 1\n",
    "            symbols=[symbol], \n",
    "            schema='mbo'  # Market by order\n",
    "        )\n",
    "        \n",
    "        # Convert to polars DataFrame\n",
    "        symbol_data = pl.from_pandas(symbol_data)\n",
    "        \n",
    "        # Filter conditions\n",
    "        symbol_data = symbol_data.filter(\n",
    "            (pl.col('publisher_id') == 2) &\n",
    "            pl.col('side').is_in(['B','A'])\n",
    "        )\n",
    "        \n",
    "        # Process each depth level\n",
    "        depths = symbol_data.select('depth').unique().to_series().to_list()\n",
    "        for depth in depths:\n",
    "            if depth != level:\n",
    "                continue\n",
    "                \n",
    "            depth_data = symbol_data.filter(pl.col('depth') == depth)\n",
    "            \n",
    "            # Calculate size differences\n",
    "            depth_data = depth_data.with_columns([\n",
    "                pl.col(f'bid_sz_0{depth}').diff().alias(f'bid_sz_0{depth}_diff'),\n",
    "                pl.col(f'ask_sz_0{depth}').diff().alias(f'ask_sz_0{depth}_diff')\n",
    "            ])\n",
    "            \n",
    "            # Apply filters\n",
    "            depth_data = depth_data.filter(\n",
    "                ~(\n",
    "                    (pl.col('action') == 'T') &\n",
    "                    (\n",
    "                        ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') != -pl.col('size'))) |\n",
    "                        ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') != -pl.col('size')))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            depth_data = depth_data.filter(\n",
    "                ~(\n",
    "                    (pl.col('action') == 'A') &\n",
    "                    (\n",
    "                        ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') != pl.col('size'))) |\n",
    "                        ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') != pl.col('size')))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            depth_data = depth_data.filter(\n",
    "                ~(\n",
    "                    (pl.col('action') == 'C') &\n",
    "                    (\n",
    "                        ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') != -pl.col('size'))) |\n",
    "                        ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') != -pl.col('size')))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            filtered_data.append(depth_data)\n",
    "    \n",
    "    # Combine all filtered data\n",
    "    df_final = pl.concat(filtered_data)\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df_final = df_final.sort('ts_event')\n",
    "    \n",
    "    # Add index columns\n",
    "    df_final = df_final.with_row_count('reindex')\n",
    "    df_final = df_final.with_columns(\n",
    "        pl.col('reindex').diff().fill_null(0).cast(pl.Int64).alias('diff_reindex')\n",
    "    )\n",
    "    \n",
    "    # Filter for GOOGL only\n",
    "    df_result = df_final.filter(\n",
    "        (pl.col('symbol') == 'GOOGL') &\n",
    "        (pl.col('depth') == level)\n",
    "    )\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Get list of DBN files\n",
    "files_parquet = glob.glob(os.path.join(\"/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240912.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240827.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240826.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241011.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240820.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240917.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240830.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241021.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240911.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240905.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240813.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240729.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240801.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240926.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240726.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240822.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240805.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240913.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241007.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240927.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241001.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240923.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241014.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240808.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241003.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240919.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240724.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241009.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240731.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240807.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240903.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241018.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240918.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241010.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240815.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241008.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240722.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240916.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240806.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240925.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240906.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240814.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240920.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240730.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240816.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240829.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240904.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240924.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240930.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241002.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240723.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240828.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240909.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241015.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241017.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240821.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241016.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240812.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240823.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240802.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240809.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240910.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240819.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20241004.parquet', '/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID/20240725.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(files_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExprDateTimeNameSpace' object has no attribute 'seconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m MBO_ \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_parquet(f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process timestamps and calculate elapsed time\u001b[39;00m\n\u001b[1;32m      8\u001b[0m MBO_ \u001b[38;5;241m=\u001b[39m MBO_\u001b[38;5;241m.\u001b[39mwith_columns([\n\u001b[1;32m      9\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_event\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrptime(pl\u001b[38;5;241m.\u001b[39mDatetime)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_event\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mts_event\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatetime\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseconds\u001b[49m()\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemps_ecoule_secondes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Filter for time window between 14:00 and 20:00\u001b[39;00m\n\u001b[1;32m     14\u001b[0m MBO_ \u001b[38;5;241m=\u001b[39m MBO_\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     15\u001b[0m     (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_event\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m     16\u001b[0m     (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_event\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ExprDateTimeNameSpace' object has no attribute 'seconds'"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "\n",
    "for f in tqdm(files_parquet):\n",
    "    # Read parquet file using polars\n",
    "    MBO_ = pl.read_parquet(f)\n",
    "    \n",
    "    # Process timestamps and calculate elapsed time\n",
    "    MBO_ = MBO_.with_columns([\n",
    "        pl.col('ts_event').str.strptime(pl.Datetime).alias('ts_event'),\n",
    "        pl.col('ts_event').str.strptime(pl.Datetime).diff().dt.seconds().alias('temps_ecoule_secondes')\n",
    "    ])\n",
    "    \n",
    "    # Filter for time window between 14:00 and 20:00\n",
    "    MBO_ = MBO_.filter(\n",
    "        (pl.col('ts_event').dt.hour() >= 14) & \n",
    "        (pl.col('ts_event').dt.hour() < 20)\n",
    "    )\n",
    "    \n",
    "    # Get unique sizes from bid and ask for depth 0\n",
    "    bid_sizes = MBO_.select('bid_sz_00').unique().to_numpy().flatten()\n",
    "    ask_sizes = MBO_.select('ask_sz_00').unique().to_numpy().flatten()\n",
    "    sizes = np.unique(np.concatenate([bid_sizes, ask_sizes]))\n",
    "    sizes.sort()\n",
    "    \n",
    "    # Drop null values\n",
    "    MBO_ = MBO_.drop_nulls()\n",
    "    \n",
    "    # Initialize dictionary\n",
    "    dic = dico_queue_size(sizes, dic)\n",
    "    \n",
    "    # Process each row\n",
    "    for row in MBO_.iter_rows(named=True):\n",
    "        taille = row['ask_sz_00'] if row['side'] == 'A' else row['bid_sz_00']\n",
    "        \n",
    "        if row['action'] == 'A':\n",
    "            dic[taille][0].append(row['temps_ecoule_secondes'])\n",
    "        elif row['action'] == 'C':\n",
    "            dic[taille][1].append(row['temps_ecoule_secondes'])\n",
    "        elif row['action'] == 'T':\n",
    "            dic[taille][2].append(row['temps_ecoule_secondes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 30, threshold=50)\n",
    "\n",
    "threshold_trade = 1000\n",
    "threshold = 40000\n",
    "for i in intensities:\n",
    "    if len(intensities[i][0])!=0:\n",
    "        if (len(intensities[i][0])>threshold):\n",
    "            Add.append(np.mean(np.array(intensities[i][0])))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(np.mean(np.array(intensities[i][1])))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(np.mean(np.array(intensities[i][2])))\n",
    "            sizes_trade.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = 1/np.array(Add), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = 1/np.array(Cancel), mode ='lines', name = f'Cancel', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = 1/np.array(Trade), mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 30, threshold=500)\n",
    "\n",
    "threshold_trade = 1000\n",
    "threshold = 20000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = np.array(Add), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = np.array(Cancel), mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = np.array(Trade), mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicu = {}\n",
    "\n",
    "for f in tqdm(files_csv):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_,1)\n",
    "    MBO_filtered_depth_0_['ts_event'] = MBO_filtered_depth_0_['ts_event'] = pd.to_datetime(MBO_filtered_depth_0_['ts_event'], errors='coerce')\n",
    "    MBO_filtered_depth_0_['temps_ecoule'] = MBO_filtered_depth_0_['ts_event'].diff()\n",
    "    MBO_filtered_depth_0_['temps_ecoule_secondes'] = MBO_filtered_depth_0_['temps_ecoule'].dt.total_seconds()\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    MBO_filtered_depth__ = MBO_filtered_depth_0_.iloc[1:] # on enleve le premier NA\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist()+(np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "\n",
    "    dicu = dico_queue_size(sizes, dicu) ## Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        if row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        if row.action =='A':\n",
    "            dicu[taille][0].append(row.temps_ecoule_secondes)\n",
    "        if row.action =='C':\n",
    "            dicu[taille][1].append(row.temps_ecoule_secondes)\n",
    "        if row.action =='T':\n",
    "            dicu[taille][2].append(row.temps_ecoule_secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dicu = remove_nan_from_dico(dicu)\n",
    "average_sizes = compute_means(dicu)\n",
    "intensities = dict(sorted(dicu.items()))\n",
    "intensities = filtrage(intensities, 30, threshold=50)\n",
    "\n",
    "threshold_trade = 1000\n",
    "threshold = 40000\n",
    "for i in intensities:\n",
    "    if len(intensities[i][0])!=0:\n",
    "        if (len(intensities[i][0])>threshold):\n",
    "            Add.append(np.mean(np.array(intensities[i][0])))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(np.mean(np.array(intensities[i][1])))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(np.mean(np.array(intensities[i][2])))\n",
    "            sizes_trade.append(i)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = 1/np.array(Add), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = 1/np.array(Cancel), mode ='lines', name = f'Cancel', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = 1/np.array(Trade), mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dicu = remove_nan_from_dico(dicu)\n",
    "average_sizes = compute_means(dicu)\n",
    "intensities = dict(sorted(dicu.items()))\n",
    "intensities = filtrage(intensities, 30, threshold=50)\n",
    "\n",
    "threshold_trade = 100\n",
    "threshold = 4000\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = 1/np.array(Add), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = 1/np.array(Cancel), mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = 1/np.array(Trade), mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "for f in tqdm(files_csv):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO = MBO_[MBO_[\"publisher_id\"] == 2]\n",
    "    MBO_filtered = MBO[MBO['symbol'] == \"GOOGL\"]\n",
    "    MBO_filtered_depth_0_ = MBO_filtered[MBO_filtered['depth'] == 1]\n",
    "    MBO_filtered_depth_0_['bid_sz_00_diff'] = MBO_filtered_depth_0_['bid_sz_00'].diff()\n",
    "    MBO_filtered_depth_0_['ask_sz_00_diff'] = MBO_filtered_depth_0_['ask_sz_00'].diff()\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[\n",
    "        ~(\n",
    "            (MBO_filtered_depth_0_['action'] == 'C')&\n",
    "            (\n",
    "                ((MBO_filtered_depth_0_['side'] == 'B')&(MBO_filtered_depth_0_['bid_sz_00_diff'] == 0)) |\n",
    "                ((MBO_filtered_depth_0_['side'] == 'A')&(MBO_filtered_depth_0_['ask_sz_00_diff'] == 0))\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    MBO_filtered_depth_0 = MBO_filtered_depth_0_.iloc[1:] # on enleve le premier NA\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0['bid_sz_00'].to_numpy())).tolist()+(np.unique(MBO_filtered_depth_0['ask_sz_00'].to_numpy())).tolist()))\n",
    "\n",
    "    MBO_filtered_depth_0['ts_event'] = pd.to_datetime(MBO_filtered_depth_0['ts_event'])\n",
    "    MBO_filtered_depth_0['temps_ecoule'] = MBO_filtered_depth_0['ts_event'].diff()\n",
    "    MBO_filtered_depth_0['temps_ecoule_secondes'] = MBO_filtered_depth_0['temps_ecoule'].dt.total_seconds()\n",
    "    MBO_filtered_depth_0 = MBO_filtered_depth_0[(MBO_filtered_depth_0['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0['ts_event'].dt.hour < 20)]\n",
    "    MBO_filtered_depth_0.dropna()\n",
    "    sizes.sort()\n",
    "\n",
    "    dic = dico_queue_size(sizes, dic) ## Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        if row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        if row.action =='A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        if row.action =='C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        if row.action =='T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 100, threshold=1000)\n",
    "\n",
    "threshold = 50000\n",
    "\n",
    "for i in intensities:\n",
    "    if len(intensities[i][0])!=0:\n",
    "        if (len(intensities[i][0])>threshold):\n",
    "            Add.append(np.mean(np.array(intensities[i][0])))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(np.mean(np.array(intensities[i][1])))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold):\n",
    "            Trade.append(np.mean(np.array(intensities[i][2])))\n",
    "            sizes_trade.append(i)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = 1/np.array(Add), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = 1/np.array(Cancel), mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = 1/np.array(Trade), mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities ASAI', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
