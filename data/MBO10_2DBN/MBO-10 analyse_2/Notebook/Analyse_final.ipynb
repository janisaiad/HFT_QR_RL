{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # ca on va devoir bcp l'uiliser\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df: pl.DataFrame, depth: int) -> pl.DataFrame:\n",
    "    # Replace 614 with 0 and filter for GOOGL symbol\n",
    "    df = df.with_columns(pl.col('*').replace(614, 0)) \\\n",
    "           .filter(pl.col('symbol') == 'GOOGL')\n",
    "    \n",
    "    # Convert timestamp and calculate diffs\n",
    "    df = df.with_columns([\n",
    "        pl.col('ts_event').str.strptime(pl.Datetime, fmt='%Y-%m-%d %H:%M:%S%.f'),\n",
    "        pl.col(f'bid_sz_0{depth}').diff().alias(f'bid_sz_0{depth}_diff'),\n",
    "        pl.col(f'ask_sz_0{depth}').diff().alias(f'ask_sz_0{depth}_diff')\n",
    "    ])\n",
    "    \n",
    "    # Filter for specific depth\n",
    "    df = df.filter(pl.col('depth') == depth)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    df = df.with_columns([\n",
    "        pl.col('ts_event').diff().alias('temps_ecoule'),\n",
    "        pl.col('ts_event').diff().dt.seconds().alias('temps_ecoule_secondes')\n",
    "    ])\n",
    "    \n",
    "    # Create status column based on conditions\n",
    "    df = df.with_columns(\n",
    "        pl.when(\n",
    "            (pl.col('action') == 'T') & (\n",
    "                ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') == -pl.col('size'))) |\n",
    "                ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') == -pl.col('size')))\n",
    "            ) |\n",
    "            (pl.col('action') == 'A') & (\n",
    "                ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') == pl.col('size'))) |\n",
    "                ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') == pl.col('size')))\n",
    "            ) |\n",
    "            (pl.col('action') == 'C') & (\n",
    "                ((pl.col('side') == 'B') & (pl.col(f'bid_sz_0{depth}_diff') == -pl.col('size'))) |\n",
    "                ((pl.col('side') == 'A') & (pl.col(f'ask_sz_0{depth}_diff') == -pl.col('size')))\n",
    "            )\n",
    "        ).then('OK').otherwise('NOK').alias('status')\n",
    "    )\n",
    "    \n",
    "    return df.filter(pl.col('status') == 'OK')\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for size in sizes:\n",
    "        if size not in dic:\n",
    "            dic[size] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for key in keys:\n",
    "        means += key * sum(len(lst) for lst in dico[key])\n",
    "        sums += sum(len(lst) for lst in dico[key])\n",
    "    return means/sums if sums > 0 else 0\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0]) < threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for key in keys:\n",
    "        real_k_index = np.argmin(np.abs(values-key))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[key][0]),\n",
    "                np.array(dico[key][1]), \n",
    "                np.array(dico[key][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[key][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[key][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[key][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "# Use glob to find all parquet files in the specified directory\n",
    "files_parquet = glob.glob(os.path.join(\"/home/janis/3A/EA/HFT_QR_RL/data/smash3/data/csv/NASDAQ/LCID\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "InvalidOperationError",
     "evalue": "conversion from `i32` to `u8` failed in column 'literal' for 1 out of 1 values: [614]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_50187/3350211117.py\", line 9, in process_file\n  File \"/tmp/ipykernel_50187/4163600372.py\", line 3, in process_dataframe\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/polars/dataframe/frame.py\", line 9194, in with_columns\n    return self.lazy().with_columns(*exprs, **named_exprs).collect(_eager=True)\n  File \"/home/janis/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/polars/lazyframe/frame.py\", line 2055, in collect\n    return wrap_df(ldf.collect(callback))\npolars.exceptions.InvalidOperationError: conversion from `i32` to `u8` failed in column 'literal' for 1 out of 1 values: [614]\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidOperationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     49\u001b[0m dic \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 50\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_parquet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m size, actions \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/3A/EA/HFT_QR_RL/.venv/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mInvalidOperationError\u001b[0m: conversion from `i32` to `u8` failed in column 'literal' for 1 out of 1 values: [614]"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    # Read parquet file with polars\n",
    "    MBO_ = pl.read_parquet(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    \n",
    "    # Filter by hour using polars expressions\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.filter(\n",
    "        (pl.col('ts_event').dt.hour() >= 14) & \n",
    "        (pl.col('ts_event').dt.hour() < 20)\n",
    "    )\n",
    "    \n",
    "    # Get unique sizes using polars and convert to float to avoid integer overflow\n",
    "    bid_sizes = MBO_filtered_depth_0_.select('bid_ct_00').unique().cast(pl.Float64).to_numpy().flatten()\n",
    "    ask_sizes = MBO_filtered_depth_0_.select('ask_ct_00').unique().cast(pl.Float64).to_numpy().flatten()\n",
    "    sizes = np.unique(np.concatenate([bid_sizes, ask_sizes]))\n",
    "    \n",
    "    # Drop nulls and sort\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.drop_nulls()\n",
    "    sizes.sort()\n",
    "    \n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    \n",
    "    # Convert to pandas for itertuples since polars doesn't have direct equivalent\n",
    "    df = MBO_filtered_depth_0_.to_pandas()\n",
    "    for row in df.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = float(row.ask_ct_00)  # Convert to float to avoid overflow\n",
    "        elif row.side == 'B':\n",
    "            taille = float(row.bid_ct_00)  # Convert to float to avoid overflow\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_parquet))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq/xnas-itch-20240723.mbp-10.csv')\n",
    "# df = df[df['symbol'] == 'GOOGL']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 100, threshold=1000)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 1000\n",
    "\n",
    "#quarter = []\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            #quarter.append(np.var(tab)*1/np.mean(tab)*len(intensities[i][0])/len(tab)*1/np.sqrt(len(tab)))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = np.array(Add)+1.96*np.array(quarter), mode ='lines', name ='Add', showlegend = True))\n",
    "#fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = np.array(Add)-1.96*np.array(quarter), mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_ct_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_ct_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_ct_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_ct_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'GOOGL']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=50)\n",
    "\n",
    "\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities GOOGL seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, depth):\n",
    "    df = df.replace(614, 0)\n",
    "    df = df[df['symbol'] == 'KHC']\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    df[f'bid_sz_0{depth}_diff'] = df[f'bid_sz_0{depth}'].diff()\n",
    "    df[f'ask_sz_0{depth}_diff'] = df[f'ask_sz_0{depth}'].diff()\n",
    "    df = df[df['depth'] == depth]\n",
    "    # df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    # try:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='coerce')\n",
    "    # except ValueError:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df['temps_ecoule'] = df['ts_event'].diff()\n",
    "    df['temps_ecoule_secondes'] = df['temps_ecoule'].dt.total_seconds()\n",
    "    condition_T = (\n",
    "        (df['action'] == 'T') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_A = (\n",
    "        (df['action'] == 'A') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_C = (\n",
    "        (df['action'] == 'C') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    df['status'] = np.where(condition_T | condition_A | condition_C, 'OK', 'NOK')\n",
    "    df = df[df['status'] == 'OK']\n",
    "    return df\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "files_csv = glob.glob(os.path.join(\"/Volumes/T9/CSV_dezippe_nasdaq\", \"*.csv\"))\n",
    "#files_csv = glob.glob(os.path.join(\"/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'KHC']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities KHC premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'KHC']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=500)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities KHC seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, depth):\n",
    "    df = df.replace(614, 0)\n",
    "    df = df[df['symbol'] == 'LCID']\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    df[f'bid_sz_0{depth}_diff'] = df[f'bid_sz_0{depth}'].diff()\n",
    "    df[f'ask_sz_0{depth}_diff'] = df[f'ask_sz_0{depth}'].diff()\n",
    "    df = df[df['depth'] == depth]\n",
    "    # df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')\n",
    "    # try:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='coerce')\n",
    "    # except ValueError:\n",
    "    #     df['ts_event'] = pd.to_datetime(df['ts_event'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df['temps_ecoule'] = df['ts_event'].diff()\n",
    "    df['temps_ecoule_secondes'] = df['temps_ecoule'].dt.total_seconds()\n",
    "    condition_T = (\n",
    "        (df['action'] == 'T') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_A = (\n",
    "        (df['action'] == 'A') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == df['size']))\n",
    "        )\n",
    "    )\n",
    "    condition_C = (\n",
    "        (df['action'] == 'C') &\n",
    "        (\n",
    "            ((df['side'] == 'B') & (df[f'bid_sz_0{depth}_diff'] == -df['size'])) |\n",
    "            ((df['side'] == 'A') & (df[f'ask_sz_0{depth}_diff'] == -df['size']))\n",
    "        )\n",
    "    )\n",
    "    df['status'] = np.where(condition_T | condition_A | condition_C, 'OK', 'NOK')\n",
    "    df = df[df['status'] == 'OK']\n",
    "    return df\n",
    "\n",
    "def dico_queue_size(sizes, dic):\n",
    "    for i in range (len(sizes)):\n",
    "        if sizes[i] not in dic:\n",
    "            dic[sizes[i]] = [[], [], []]\n",
    "    return dic\n",
    "\n",
    "def compute_means(dico):\n",
    "    sums = 0\n",
    "    means = 0\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    for i in range (len(keys)):\n",
    "        means = means+keys[i]*len(dico[keys[i]][0])+keys[i]*len(dico[keys[i]][1])+keys[i]*len(dico[keys[i]][2])\n",
    "        sums = sums+len(dico[keys[i]][0])+len(dico[keys[i]][1])+len(dico[keys[i]][2])\n",
    "    return means/sums\n",
    "\n",
    "def filtrage(dico, nombre_bins, threshold=100):\n",
    "    dico_p = dict(reversed(list(dico.items())))\n",
    "    keys = list(dico_p.keys())\n",
    "    i = 0\n",
    "    while len(dico_p[keys[i]][0])<threshold:\n",
    "        i += 1\n",
    "    values = np.linspace(0, keys[i], nombre_bins, endpoint=True)\n",
    "    keys = np.array(list(dico.keys()))\n",
    "    \n",
    "    real_dic = {}\n",
    "    for i in range (len(keys)):\n",
    "        real_k_index = np.argmin(np.abs(values-keys[i]))\n",
    "        real_k = values[real_k_index]\n",
    "        \n",
    "        if real_k not in real_dic:\n",
    "            real_dic[real_k] = [\n",
    "                np.array(dico[keys[i]][0]),\n",
    "                np.array(dico[keys[i]][1]),\n",
    "                np.array(dico[keys[i]][2])\n",
    "            ]\n",
    "        else:\n",
    "            real_dic[real_k] = [\n",
    "                np.concatenate([real_dic[real_k][0], dico[keys[i]][0]]),\n",
    "                np.concatenate([real_dic[real_k][1], dico[keys[i]][1]]),\n",
    "                np.concatenate([real_dic[real_k][2], dico[keys[i]][2]])\n",
    "            ]\n",
    "    return real_dic\n",
    "\n",
    "def remove_nan_from_dico(dico):\n",
    "    cleaned_dico = {}\n",
    "    for key, value_lists in dico.items():\n",
    "        cleaned_value_lists = []\n",
    "        for value_list in value_lists:\n",
    "            value_array = np.array(value_list)\n",
    "            cleaned_array = value_array[~np.isnan(value_array)]\n",
    "            cleaned_value_lists.append(cleaned_array.tolist())\n",
    "        cleaned_dico[key] = cleaned_value_lists\n",
    "    return cleaned_dico\n",
    "\n",
    "files_csv = glob.glob(os.path.join(\"/Volumes/T9/CSV_dezippe_nasdaq\", \"*.csv\"))\n",
    "#files_csv = glob.glob(os.path.join(\"/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 0\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'LCID']\n",
    "# df = df[df['depth'] == 0]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 4000\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities LUCID premiere limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(f, level, dic):\n",
    "    MBO_ = pd.read_csv(f)\n",
    "    MBO_filtered_depth_0_ = process_dataframe(MBO_, level)\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_[(MBO_filtered_depth_0_['ts_event'].dt.hour >= 14) & (MBO_filtered_depth_0_['ts_event'].dt.hour < 20)]\n",
    "    sizes = np.unique(np.array((np.unique(MBO_filtered_depth_0_['bid_sz_00'].to_numpy())).tolist() + (np.unique(MBO_filtered_depth_0_['ask_sz_00'].to_numpy())).tolist()))\n",
    "    MBO_filtered_depth_0_ = MBO_filtered_depth_0_.dropna()\n",
    "    sizes.sort()\n",
    "    dic = dico_queue_size(sizes, dic)  # Add, Cancel, Trade\n",
    "    for row in MBO_filtered_depth_0_.itertuples():\n",
    "        if row.side == 'A':\n",
    "            taille = row.ask_sz_00\n",
    "        elif row.side == 'B':\n",
    "            taille = row.bid_sz_00\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if row.action == 'A':\n",
    "            dic[taille][0].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'C':\n",
    "            dic[taille][1].append(row.temps_ecoule_secondes)\n",
    "        elif row.action == 'T':\n",
    "            dic[taille][2].append(row.temps_ecoule_secondes)\n",
    "    \n",
    "    return dic\n",
    "level = 1\n",
    "\n",
    "dic = {}\n",
    "results = Parallel(n_jobs=4)(delayed(process_file)(f, level, dic) for f in tqdm(files_csv))\n",
    "\n",
    "for result in results:\n",
    "    for size, actions in result.items():\n",
    "        if size not in dic:\n",
    "            dic[size] = actions\n",
    "        else:\n",
    "            for i in range(3):  # Add, Cancel, Trade\n",
    "                dic[size][i].extend(actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "Add = []\n",
    "Cancel = []\n",
    "Trade = []\n",
    "sizes_add = []\n",
    "sizes_cancel = []\n",
    "sizes_trade = []\n",
    "threshold = 100\n",
    "\n",
    "dic = remove_nan_from_dico(dic)\n",
    "average_sizes = compute_means(dic)\n",
    "# df = pd.read_csv('/Users/edouard/Desktop/EA p1  HFT/HFT_QR_RL_save/Sans titre/HFT_QR_RL/data/MBO-10 analyse_2/CSV_dezippe_nasdaq')\n",
    "# df = df[df['symbol'] == 'LCID']\n",
    "# df = df[df['depth'] == 1]\n",
    "# df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "# df = df[(df['ts_event'].dt.hour >= 14) & (df['ts_event'].dt.hour < 20)]\n",
    "# average_sizes = np.mean(df['size'].to_numpy())\n",
    "intensities = dict(sorted(dic.items()))\n",
    "intensities = filtrage(intensities, 50, threshold=300)\n",
    "\n",
    "threshold_trade = 2000\n",
    "threshold = 100\n",
    "\n",
    "for i in intensities:\n",
    "    tab = np.concatenate((intensities[i][0], intensities[i][1], intensities[i][2]))\n",
    "    if (len(intensities[i][0])>threshold):\n",
    "            Add.append(1/np.mean(tab)*len(intensities[i][0])/len(tab))\n",
    "            sizes_add.append(i)\n",
    "    if len(intensities[i][1])!=0:\n",
    "        if (len(intensities[i][1])>threshold):\n",
    "            Cancel.append(1/np.mean(tab)*len(intensities[i][1])/len(tab))\n",
    "            sizes_cancel.append(i)\n",
    "    if len(intensities[i][2])!=0:\n",
    "        if (len(intensities[i][2])>threshold_trade):\n",
    "            Trade.append(1/np.mean(tab)*len(intensities[i][2])/len(tab))\n",
    "            sizes_trade.append(i)\n",
    "            \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = sizes_add/average_sizes, y = Add, mode ='lines', name ='Add', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_cancel/average_sizes, y = Cancel, mode ='lines', name = f'Cancel', showlegend = True))\n",
    "fig.add_trace(go.Scatter(x = sizes_trade/average_sizes, y = Trade, mode ='lines', name = f'Trade', showlegend = True))\n",
    "fig.update_layout(title=f'Intensities LUCID seconde limite', xaxis_title='size', yaxis_title='intensity', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
